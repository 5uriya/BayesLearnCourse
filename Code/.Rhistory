rep(d,1,4)
rep(t(d),4)
rep(d,4,43)
rep(d,4,3)
rep(d,3,4)
?rep
?cbind
?rep
rep(d,3,each=4)
rep(d,3,each=4, byrow)
rep(d,c(1,3))
rep(d,c(4,3))
rep(c(1,3,5,7),4)
matrix(rep(c(1,3,5,7),4))
matrix(rep(c(1,3,5,7),4),4,3)
matrix(rep(c(1,3,5,7),4),4,4)
matrix(rep(d,4),4,4)
matrix(rep(d,nd),nd,nd)
invsigmas <- c(1,3,5,7)
n <- length(invsigmas)
0.9*0.0001/(0.9*0.0001+0.05*(1-0.0001)
)
0.9*0.0001/(0.9*0.0001+0.05*(1-0.0001))
0.9*0.0001/(0.9*0.0001+0.05*(1-0.0001))
0.001796945/0.0001
NormalNonInfoPrior<-function(NDraws,Data){
#####################################################################################
# PURPOSE:   Generates samples from the joint posterior distribution of the parameters in the
#
#    	MODEL: x1,....xn iid Normal(mu,sigma^2) model.
#		PRIOR: p(mu,sigma^2) propto 1/sigma^2
#
#
# INPUT:	NDraw: 		(Scalar)	The number of posterior draws
#		Data:		(n-by-1)	Data vector of n observations
#
# OUTPUT:	PostDraws:	(NDraws-by-2)	Matrix of posterior draws.
#						First column holds draws of mu
#						Second column holds draws of sigma^2
#
# AUTHOR:	Mattias Villani, Sveriges Riksbank and Stockholm University.
#		E-mail: mattias.villani@riksbank.se.
#
# DATE:		2005-04-13
#
######################################################################################
Datamean<-mean(Data)
s2<-var(Data)
n<-length(Data)
PostDraws=matrix(0,NDraws,2)
PostDraws[,1]<-((n-1)*s2)/rchisq(NDraws,n-1)
PostDraws[,2]<-Datamean+rnorm(NDraws,0,1)*sqrt(PostDraws[,1]/n)
return(PostDraws)
}
Data<-rnorm(100,5,10) 			# Sampling 100 observations from the N(5,10) density##
PostDraws<-NormalNonInfoPrior(100,Data) # Generating 1000 draws from the joint posterior density of mu and sigma^2
hist(PostDraws[,1]) 			# Plotting the histogram of mu-draws
NormalNonInfoPrior
x <- rnorm(100,mean = 3)
hist(x)
NormalNonInfoPrior(1000, x)
PostDraws <- NormalNonInfoPrior(1000, x)
hist(PostDraws[,1],50)
hist(PostDraws[,2],50)
PostDraws <- NormalNonInfoPrior(10000, x)
hist(PostDraws[,1],50)
CV <- PostDraws[,1]/PostDraws[,2]
CV
hist(CV,50)
dpois(0,lambda = 1)
dpois(c(0,1,2,3,4,5,6),lambda = 1)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
LogScaledInvChi2 <- function(x, v, tau){
(v/2)*log(tau^2*v/2) - lgamma(v/2) - (v*tau^2)/(2*x) - (1 + v/2)*log(x)
}
LogVonMisesPDF <- function(x, mu, kappa){
kappa*cos(x-mu) - log(2*pi) - log(besselI(kappa, 0))
}
logPosterior <- function(kappa, x, mu, v, tau){
sum(LogVonMisesPDF(x,mu,kappa)) + sum(LogScaledInvChi2(x, v, tau))
}
windDirections <- c(0.6981317, 5.2883476,5.6897734,4.9741884,5.1661746,5.4803339,0.3490659,
5.3756141, 5.2185345, 5.1661746) - pi
kappaVals <- seq(0.01,10,by = 0.01)
logPosts <- matrix(0,length(kappaVals),1)
count <- 0
for (kappa in kappaVals){
count <- count + 1
logPosts[count] <- logPosterior(kappa, x = windDirections, mu = 5.536801, v = 1, tau = 0.001)
}
plot(kappaVals, exp(logPosts), type = "l")
LogScaledInvChi2 <- function(x, v, tau){
(v/2)*log(tau^2*v/2) - lgamma(v/2) - (v*tau^2)/(2*x) - (1 + v/2)*log(x)
}
LogVonMisesPDF <- function(x, mu, kappa){
kappa*cos(x-mu) - log(2*pi) - log(besselI(kappa, 0))
}
logPosterior <- function(kappa, x, mu, v, tau){
sum(LogVonMisesPDF(x,mu,kappa)) + sum(LogScaledInvChi2(x, v, tau))
}
windDirections <- c(0.6981317, 5.2883476,5.6897734,4.9741884,5.1661746,5.4803339,0.3490659,
5.3756141, 5.2185345, 5.1661746) - pi
kappaVals <- seq(0.01,10,by = 0.01)
logPosts <- matrix(0,length(kappaVals),1)
count <- 0
for (kappa in kappaVals){
count <- count + 1
logPosts[count] <- logPosterior(kappa, x = windDirections, mu = 5.536801, v = 1, tau = 0.001)
}
plot(kappaVals, exp(logPosts), type = "l")
plot(kappaVals, exp(logPosts -mean(logPosts)), type = "l")
logPosts
windDirections <- c(0.6981317, 5.2883476,5.6897734,4.9741884,5.1661746,5.4803339,0.3490659,
5.3756141, 5.2185345, 5.1661746) - pi
kappaVals <- seq(0.01,10,by = 0.01)
logPosts <- matrix(0,length(kappaVals),1)
count <- 0
for (kappa in kappaVals){
count <- count + 1
logPosts[count] <- logPosterior(kappa, x = windDirections, mu = 5.536801, v = 1, tau = 0.001)
}
plot(kappaVals, exp(logPosts), type = "l")
windDirections
kappaVals <- seq(0.01,10,by = 0.01)
logPosts <- matrix(0,length(kappaVals),1)
count <- 0
for (kappa in kappaVals){
count <- count + 1
logPosts[count] <- logPosterior(kappa, x = windDirections, mu = 5.536801, v = 1, tau = 0.001)
}
logPosterior <- function(kappa, x, mu, v, tau){
sum(LogVonMisesPDF(x,mu,kappa)) + sum(LogScaledInvChi2(x, v, tau))
}
LogVonMisesPDF <- function(x, mu, kappa){
kappa*cos(x-mu) - log(2*pi) - log(besselI(kappa, 0))
}
logPosterior <- function(kappa, x, mu, v, tau){
sum(LogVonMisesPDF(x,mu,kappa)) + sum(LogScaledInvChi2(x, v, tau))
}
kappaVals <- seq(0.01,10,by = 0.01)
logPosts <- matrix(0,length(kappaVals),1)
count <- 0
for (kappa in kappaVals){
count <- count + 1
logPosts[count] <- logPosterior(kappa, x = windDirections, mu = 5.536801, v = 1, tau = 0.001)
}
plot(kappaVals, exp(logPosts), type = "l")
logPosts
windDirections <- c(0.6981317, 5.2883476, 5.6897734, 4.9741884, 5.1661746, 5.4803339, 0.3490659,
5.3756141, 5.2185345, 5.1661746) - pi
windDirections
kappaVals <- seq(0.01,10,by = 0.01)
logPosts <- matrix(0,length(kappaVals),1)
count <- 0
for (kappa in kappaVals){
count <- count + 1
logPosts[count] <- logPosterior(kappa, x = windDirections, mu = 2.39, v = 1, tau = 0.001)
}
plot(kappaVals, exp(logPosts), type = "l")
?decp
?dexp
logPosterior <- function(kappa, x, mu, v, tau){
sum(LogVonMisesPDF(x,mu,kappa)) + dexp(kappa,rate=1,log = TRUE)
}
windDirections <- c(0.6981317, 5.2883476, 5.6897734, 4.9741884, 5.1661746, 5.4803339, 0.3490659,
5.3756141, 5.2185345, 5.1661746) - pi
kappaVals <- seq(0.01,10,by = 0.01)
logPosts <- matrix(0,length(kappaVals),1)
count <- 0
for (kappa in kappaVals){
count <- count + 1
logPosts[count] <- logPosterior(kappa, x = windDirections, mu = 2.39, v = 1, tau = 0.001)
}
plot(kappaVals, exp(logPosts), type = "l")
#install.packages("CircStats") # Includes function for the MLE of rho and kappa in vonMises.
library(CircStats)
LogScaledInvChi2 <- function(x, v, tau){
(v/2)*log(tau^2*v/2) - lgamma(v/2) - (v*tau^2)/(2*x) - (1 + v/2)*log(x)
}
LogVonMisesPDF <- function(x, mu, kappa){
kappa*cos(x-mu) - log(2*pi) - log(besselI(kappa, 0))
}
logPosterior <- function(kappa, x, mu){
sum(LogVonMisesPDF(x,mu,kappa)) + dexp(kappa,rate=1,log = TRUE)
}
windDirections <- c(0.6981317, 5.2883476, 5.6897734, 4.9741884, 5.1661746, 5.4803339, 0.3490659,
5.3756141, 5.2185345, 5.1661746) - pi
kappaVals <- seq(0.01,10,by = 0.01)
logPosts <- matrix(0,length(kappaVals),1)
count <- 0
for (kappa in kappaVals){
count <- count + 1
logPosts[count] <- logPosterior(kappa, x = windDirections, mu = 2.39, v = 1, tau = 0.001)
}
plot(kappaVals, exp(logPosts), type = "l")
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
gamma(59)
gamma(4.3)
gamma(4.3)-4.3*gamma(3.3)
gamma(4.3)-3.3*gamma(3.3)
gamma(4.3+1)-4.3*gamma(4.3)
gamma(1/20)
gamma(1)
gamma(0.9)
gamma(0.3)
gamma(0.1)
gamma(0.001)
gamma(1)
gamma(1/20)
gamma(1/20)/gamma(10000)
gamma(1/20)/gamma(100)
(gamma(1/20)/gamma(100+1/20))/(gamma(100))
(gamma(1/20)/gamma(100+1/20))/(gamma(100))
(gamma(1/20)/gamma(100+1/20))/(1/gamma(100))
1
(gamma(1/20)/gamma(100+1/200))/(1/gamma(100))
(1/gamma(100+1/200))/(1/gamma(100))
(1/gamma(100+1/2))/(1/gamma(100))
alpha <- 1/K,gamma(alpha)
alpha <- 1/K;gamma(alpha)
K<-20;alpha <- 1/K;gamma(alpha)
K<-20;alpha <- 1/K;gamma(alpha)/gamma(m+alpha)
m <- 1000;K<-20;alpha <- 1/K;gamma(alpha)/gamma(m+alpha)
?gammaln
?gamma
lgamma
m <- 1000;K<-20;alpha <- 1/K;lgamma(alpha)/lgamma(m+alpha)
m <- 1000;K<-20;alpha <- 1/K;lgamma(alpha)-lgamma(m+alpha)+lgamma(m)
m <- 1000;K<-20;alpha <- 1/K;c(lgamma(alpha),lgamma(alpha)-lgamma(m+alpha)+lgamma(m))
m <- 10000;K<-20;alpha <- 1/K;c(lgamma(alpha),lgamma(alpha)-lgamma(m+alpha)+lgamma(m))
m <- 100;K<-20;alpha <- 1/K;c(lgamma(alpha),lgamma(alpha)-lgamma(m+alpha)+lgamma(m))
m <- 10000;K<-20;alpha <- 1/K;c(lgamma(alpha),lgamma(alpha)-lgamma(m+alpha)+lgamma(m))
m <- 100000;K<-20;alpha <- 1/K;c(lgamma(alpha),lgamma(alpha)-lgamma(m+alpha)+lgamma(m))
m <- 1000000;K<-20;alpha <- 1/K;c(lgamma(alpha),lgamma(alpha)-lgamma(m+alpha)+lgamma(m))
lgamma(m+alpha)
alpha
lgamma(m+alpha)-lgamma(m)
m<-10;lgamma(m+alpha)-lgamma(m)
m<-100;lgamma(m+alpha)-lgamma(m)
i<-0;for (i in c(10,100,1000,10000,100000,1000000)){;i <- i+1;a[i]<-lgamma(m+alpha)-lgamma(m)}
a <- matrix(NA,6,1)
a
i<-0;for (i in c(10,100,1000,10000,100000,1000000)){;i <- i+1;a[i]<-lgamma(m+alpha)-lgamma(m)}
a
i<-0;for (m in c(10,100,1000,10000,100000,1000000)){;i <- i+1;a[i]<-lgamma(m+alpha)-lgamma(m)}
i<-0;for (m in c(10,100,1000,10000,100000,1000000)){;i <- i+1;a[i]<-lgamma(m+alpha)-lgamma(m)}
a
a <- matrix(NA,6,1)
i<-0;for (m in c(10,100,1000,10000,100000,1000000)){;i <- i+1;a[i]<-lgamma(m+alpha)-lgamma(m)}
a
K
1/K
diff
diff(a)
i<-0;for (m in c(10,100,1000,10000,100000,1000000)+0.5){;i <- i+1;a[i]<-lgamma(m+alpha)-lgamma(m)}
diff(a)
a
m
i<-0;for (m in c(10,100,1000,10000,100000,1000000)+2.5){;i <- i+1;a[i]<-lgamma(m+alpha)-lgamma(m)}
m
diff(a)
rbeta(2,5)
rbeta(10,2,5)
rbeta(10,20,50)
rbeta(10,200,500)
x <- rbeta(100000,a,((1-c)/c)*a)
c <- 0.2;a <-10;x <- rbeta(100000,a,((1-c)/c)*a)
c <- 0.2;a <-10;x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),var(x))
c <- 0.2;a <-10;x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),var(x),(1-c)*c^2)
c <- 0.2;a <-10;x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),var(x),(1-c)*c^2/(a+c))
c <- 0.2;a <-10;x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),var(x),(1-c)*c^2/(a+c))
c <- 0.2;a <-100;x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),var(x),(1-c)*c^2/(a+c))
c <- 0.8;a <-100;x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),var(x),(1-c)*c^2/(a+c))
i<-1;c <- 0.8;a <-log(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),var(x),(1-c)*c^2/(a+c))
log(1)
i<-1;c <- 0.8;a <-5+log(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),var(x),(1-c)*c^2/(a+c))
i<-10;c <- 0.8;a <-5+log(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),var(x),(1-c)*c^2/(a+c))
i<-10;c <- 0.2;a <-5+log(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),var(x),(1-c)*c^2/(a+c))
i<-10;c <- 0.2;a <-5+log(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),var(x),(1-c)*c^2/(a+c))
log(10)
i<-10;c <- 0.2;a <-5;x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),var(x),(1-c)*c^2/(a+c))
i<-10;c <- 0.2;a <-5;x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),std(x),sqrt((1-c)*c^2/(a+c)))
i<-10;c <- 0.2;a <-5;x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),sd(x),sqrt((1-c)*c^2/(a+c)))
log(1000)
log(10)
log(50)
log(100)
log(1000)
log(10000)
i<-10;c <- 0.2;a <-log(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),sd(x),sqrt((1-c)*c^2/(a+c)))
i<-100;c <- 0.2;a <-log(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),sd(x),sqrt((1-c)*c^2/(a+c)))
i<-1000;c <- 0.2;a <-log(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),sd(x),sqrt((1-c)*c^2/(a+c)))
sqrt(100)
c(sqrt(i),log(i))
i <- 100;c(sqrt(i),log(i))
i <- 1000;c(sqrt(i),log(i))
i<-1000;c <- 0.2;a <-sqrt(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),sd(x),sqrt((1-c)*c^2/(a+c)))
i<-10;c <- 0.2;a <-sqrt(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),sd(x),sqrt((1-c)*c^2/(a+c)))
i<-10;c <- 0.2;a <-sqrt(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),sd(x),sqrt((1-c)*c^2/(a+c)),a,((1-c)/c)*a)
i<-100;c <- 0.2;a <-sqrt(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),sd(x),sqrt((1-c)*c^2/(a+c)),a,((1-c)/c)*a)
i<-1000;c <- 0.2;a <-sqrt(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),sd(x),sqrt((1-c)*c^2/(a+c)),a,((1-c)/c)*a)
i<-10;c <- 0.2;a <-sqrt(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),sd(x),sqrt((1-c)*c^2/(a+c)),a,((1-c)/c)*a)
i<-100;c <- 0.2;a <-sqrt(i);x <- rbeta(100000,a,((1-c)/c)*a);c(mean(x),sd(x),sqrt((1-c)*c^2/(a+c)),a,((1-c)/c)*a)
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
install.packages("geoR") # We need the rinvchisq() function for simulating random draws from the scaled inv chi-square.
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
x
hist(x)
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
# Setting up the plot
xGrid <- seq(min(x)-1*apply(x,2,sd),max(x)+1*apply(x,2,sd),length = 100)
xGridMin <- min(xGrid)
xGridMax <- max(xGrid)
mixDensMean <- rep(0,length(xGrid))
effIterCount <- 0
ylim <- c(0,2*max(hist(x)$intensities))
# Setting up the plot
xGrid <- seq(min(x)-1*apply(x,2,sd),max(x)+1*apply(x,2,sd),length = 100)
xGridMin <- min(xGrid)
xGridMax <- max(xGrid)
mixDensMean <- rep(0,length(xGrid))
effIterCount <- 0
ylim <- c(0,2*max(hist(x)$intensities))
# Setting up the plot
xGrid <- seq(min(x)-1*apply(x,2,sd),max(x)+1*apply(x,2,sd),length = 100)
xGridMin <- min(xGrid)
xGridMax <- max(xGrid)
mixDensMean <- rep(0,length(xGrid))
effIterCount <- 0
ylim <- c(0,2*max(hist(x)$intensities))
hist(x)$intensities)''
hist(x)$intensities)
hist(x)$intensities
hist(x)
hist(x)$
''
2*max(hist(x)$intensities)
a <- hist(x)
a
ylim <- c(0,2*max(hist(x)$density))
ylim
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/NormalMixtureGibbs.R')
source('~/Dropbox/Teaching/BayesLearning/Code/SimulateDiscreteMarkovChain.R')
source('~/Dropbox/Teaching/BayesLearning/Code/SimulateDiscreteMarkovChain.R')
source('~/Dropbox/Teaching/BayesLearning/Code/SimulateDiscreteMarkovChain.R')
nIter <- 10000 # Number of simulated steps of the Markov chain
P = matrix(c(0.8,0.1,0.1,0.2,0.6,0.2,0.3,0.3,0.4),3,3, byrow=TRUE) # Define a transition matrix
#####   END USER INPUT    #####
install.packages("expm") # To get the matrix power
library(expm)
lineColors = c("red","blue","green","black","yellow")
nStates <- dim(P)[1]
print(P)
# Stationary distribution is normalized left eigenvector of P corresponding to eigenvalue of one.
eigObj <- eigen(t(P))
eigenVect <- eigObj$vector[,eigObj$values == 1]
piStat <- eigenVect/sum(eigenVect)
P
piStat
P
piStat
print(P%^%100)
print(P%^%4)
rm(list=ls())
library(rstan)
nBurnin <- 1000
nIter <- 2000
nBurnin <- 1000
nIter <- 2000
rstanSeedModel<-'
data {
int<lower=0> N; ## Number of observations
int<lower=0> r[N]; ## Number of successes
int<lower=0> n[N]; ## Numer of trials
vector[N] x1; ## Covariate 1
vector[N] x2; ## Covariate 2
}
parameters {
real alpha0;
real alpha1;
real alpha2;
real<lower=0> tau;
vector[N] b;
}
transformed parameters {
real<lower=0> sigma;
sigma <- 1.0 / sqrt(tau);
}
model {
## Priors
alpha0 ~ normal(0.0,1.0E3);
alpha1 ~ normal(0.0,1.0E3);
alpha2 ~ normal(0.0,1.0E3);
tau ~ gamma(1.0E-3,1.0E-3);
## Model
b ~ normal(0.0, sigma);
r ~ binomial_logit(n, alpha0 + alpha1 * x1 + alpha2 * x2 + b);
}
'
rstanSeedModel
seedsData <- list(N = 21,
r = c(10, 23, 23, 26, 17, 5, 53, 55, 32, 46, 10, 8, 10, 8, 23, 0, 3, 22, 15, 32, 3),
n = c(39, 62, 81, 51, 39, 6, 74, 72, 51, 79, 13, 16, 30, 28, 45, 4, 12, 41, 30, 51, 7),
x1 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1),
x2 = c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1))
seedsData
fit1<-stan(model_code=rstanSeedModel,
data=seedsData,
warmup=nBurnin,
iter=(nBurnin+nIter),
chains=2)
print(fit1,digits_summary=3)
plot(fit1)
sqrt(10)
sqrt(100)
sqrt(1000)
sqrt(10000)
sqrt(1000000)
sigma <- 1
log(1)
v0 = 10^-4;v1<-100;log(v1/v0)/(1/v0-1/v1)
v0 = 10^-3;v1<-100;log(v1/v0)/(1/v0-1/v1)
v0 = 10^-2;v1<-100;log(v1/v0)/(1/v0-1/v1)
v0 = 10^-1;v1<-100;log(v1/v0)/(1/v0-1/v1)
v0 = 10^-0;v1<-100;log(v1/v0)/(1/v0-1/v1)
?multinomial
?categorical
rmultinom(n,prob=c(0.4,0.3,0.2))
n<- 50;x <-rmultinom(n,prob=c(0.4,0.3,0.2))
n<- 50;x <-rmultinom(n,size = 3, prob=c(0.4,0.3,0.2))
x
n<- 50;x <-rmultinom(1,size = 50, prob=c(0.4,0.3,0.2))
x
x
x
n<- 50;x <-rmultinom(1,size = 50, prob=c(0.4,0.3,0.2));x
n<- 50;x <-rmultinom(1,size = 50, prob=c(0.4,0.3,0.2));x
n<- 50;x <-rmultinom(1,size = 50, prob=c(0.4,0.3,0.2));x
n<- 50;x <-rmultinom(1,size = 50, prob=c(0.4,0.3,0.2));x
n<- 50;x <-rmultinom(1,size = 50, prob=c(0.4,0.3,0.2));x
n<- 50;x <-rmultinom(1,size = 50, prob=c(0.4,0.3,0.2));x
n<- 50;x <-rmultinom(1000,size = 50, prob=c(0.4,0.3,0.2));x
dim(x)
hist(x[1,])
dat <- read.dta("http://www.ats.ucla.edu/stat/stata/dae/nb_data.dta")
dat <- read.table("http://www.ats.ucla.edu/stat/stata/dae/nb_data.dta")
dat <- read.table("http://www.ats.ucla.edu/stat/stata/dae/nb_data.dta")
zinb <- read.csv("http://www.ats.ucla.edu/stat/data/fish.csv")
zinb
names(zinb)
hist
hist(count)
names(zinb)
hist(zinb$count)
hist(zinb$count)
annual = read.table("http://myweb.fsu.edu/jelsner/data/AnnualData.txt", header = TRUE)
annual
names(annual)
dat = subset(annual, Year >= 1950)
df = data.frame(H = dat$B.1, SOI = dat$soi, SST = dat$sst)
df
plot(dat)
df
plot(df$SOI,df$H)
plot(df$SST,df$H)
prm = glm(H ~ SOI + SST, data = df, family = "poisson")
prm
summary(prm)
setwd('Dropbox/Teaching/BayesLearning/Code/')
data <- read.table('eBayNumberOfBidderData.dat')
head(data)
?glm
glm(nBids ~ LogBook + MinBidShare, data = data)
data
glm(nBids ~ LogBook + MinBidShare, data = data)
names(data)
data
data <- read.table('eBayNumberOfBidderData.dat')
names(data)
data <- read.table('eBayNumberOfBidderData.dat',header=True)
data <- read.table('eBayNumberOfBidderData.dat',header=TRUE)
names(data)
glm(nBids ~ LogBook + MinBidShare, data = data)
glm(nBids ~ LogBook + MinBidShare + Const, data = data)
glm(nBids ~ LogBook + MinBidShare + Const, data = data, constant = )
data$MinBidShare
