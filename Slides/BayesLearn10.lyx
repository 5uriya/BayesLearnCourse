#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass beamer
\begin_preamble
\setcounter{MaxMatrixCols}{10}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathpazo}
\usepackage{hyperref}
\usepackage{multimedia}
\usepackage{xcolor}
\usepackage{colortbl}
\definecolor{RawSienna}{cmyk}{0,0.87,0.82,0.31}
\definecolor{gray97}{cmyk}{0,0,0,0.03}
\definecolor{robinsegg}{cmyk}{0.18,0.04,0,0.07}
\definecolor{cola}{cmyk}{0,0.315,0.35,0.155}

\newenvironment{stepenumerate}{\begin{enumerate}[<+->]}{\end{enumerate}}
\newenvironment{stepitemize}{\begin{itemize}[<+->]}{\end{itemize} }
\newenvironment{stepenumeratewithalert}{\begin{enumerate}[<+-| alert@+>]}{\end{enumerate}}
\newenvironment{stepitemizewithalert}{\begin{itemize}[<+-| alert@+>]}{\end{itemize} }
\usecolortheme[named=RawSienna]{structure}
%\usecolortheme[RGB={205,0,0}]{structure}
\setbeamertemplate{navigation symbols}{}
\useoutertheme{infolines}
\usetheme{default}
\setbeamertemplate{blocks}[shadow=true]
%\setbeamerfont{structure}{shape=\itshape}
\usefonttheme{structuresmallcapsserif}
\setbeamertemplate{background canvas}{
 % \ifnum \thepage>0 \relax % we are on the first page
%\includegraphics[width=\paperwidth,height=\paperheight]{/home/mv/Dropbox/Foton/IconsWallpaper/greyribbonLighter.jpg}
 % \else
 	% No background for page 2 and onwards
 % \fi
}
\end_preamble
\options xcolor=svgnames, handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Bayesian Learning
\end_layout

\end_inset

Bayesian Learning - Lecture 10 and 11
\end_layout

\begin_layout Author
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Mattias Villani
\end_layout

\end_inset

Mattias Villani
\end_layout

\begin_layout Institute

\series bold
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\series bold
Statistics, LiU
\end_layout

\end_inset

Division of Statistics and Machine Learning
\begin_inset Newline newline
\end_inset

Department of Computer and Information Science
\begin_inset Newline newline
\end_inset

Link√∂ping University 
\end_layout

\begin_layout Date
\begin_inset space \thinspace{}
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Overview
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Model comparison
\end_layout

\begin_layout Itemize
Model checking
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Likelihood is no good for model comparison
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider two models: 
\begin_inset Formula $M_{1}$
\end_inset

 and 
\begin_inset Formula $M_{2}$
\end_inset

.
 Let :
\begin_inset Formula $p_{i}(y|\theta_{i})$
\end_inset

 denote the data density under model 
\begin_inset Formula $M_{i}$
\end_inset

.
 If we knew the values of 
\begin_inset Formula $\theta_{1}$
\end_inset

 and 
\begin_inset Formula $\theta_{2}$
\end_inset

, then the likelihood ratio 
\begin_inset Formula 
\[
\frac{p_{1}(y|\theta_{1})}{p_{2}(y|\theta_{2})},
\]

\end_inset

could be used to compare the models.
\end_layout

\begin_layout Itemize
What if the model parameters are unknown? The estimated likelihood ratio:
\begin_inset Formula 
\[
\frac{p_{1}(y|\hat{\theta}_{1})}{p_{2}(y|\hat{\theta}_{2})}.
\]

\end_inset

where 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

 are the maximum likelihood estimates.
\end_layout

\begin_layout Itemize
Estimated likelihood ratio is useless in itself as the larger model always
 has larger likelihood.
 Comparison with sampling distribution of the estimated likelihood ratio
 is one solution.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Enter Bayes
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Bayesian: use your priors 
\begin_inset Formula $p_{1}(\theta_{1})$
\end_inset

 och 
\begin_inset Formula $p_{2}(\theta_{2})$
\end_inset

 and compute the 
\series bold
marginal likelihood
\series default
, or 
\series bold
prior predictive density
\series default
, for each model
\begin_inset Formula 
\[
p_{k}(y)=\int p_{k}(y|\theta_{k})p_{k}(\theta_{k})d\theta_{k}.
\]

\end_inset


\end_layout

\begin_layout Itemize
The 
\series bold
Bayes factor
\series default
 can be used to compare to models
\begin_inset Formula 
\[
B_{12}(y)=\frac{p_{1}(y)}{p_{2}(y)}.
\]

\end_inset


\end_layout

\begin_layout Itemize
The marginal likelihoods may be converted into posterior probabilities of
 the models (
\begin_inset Formula $M_{1},M_{2}$
\end_inset

):
\begin_inset Formula 
\[
\frac{p(M_{1}|y)}{p(M_{2}|y)}=\frac{p(M_{1})}{p(M_{2})}B_{12}(y),
\]

\end_inset

where 
\begin_inset Formula $B_{12}(y)$
\end_inset

 is the Bayes factor in favor of 
\begin_inset Formula $M_{1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\text{Posterior model odds ratio = Prior model odds ratio }\cdot\text{Bayes factor}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bayesian hypothesis testing - Bernoulli
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Hypothesis testing
\series default
 is just a special case of model selection: 
\begin_inset Formula 
\begin{align*}
M_{0}: & x_{1},...,x_{n}\overset{iid}{\sim}Bernoulli(\theta_{0})\\
M_{1}: & x_{1},...,x_{n}\overset{iid}{\sim}Bernoulli(\theta),\text{}\theta\sim Beta(\alpha,\beta)
\end{align*}

\end_inset


\begin_inset Formula 
\[
p(x_{1},...,x_{n}|M_{0})=\theta_{0}^{s}(1-\theta_{0})^{f},
\]

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
p(x_{1},...,x_{n}|M_{1}) & = & \int_{0}^{1}\theta^{s}(1-\theta)^{f}B(\alpha,\beta)^{-1}\theta^{\alpha-1}(1-\theta)^{\beta-1}d\theta\\
 & = & B(\alpha+s,\beta+f)/B(\alpha,\beta),
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $B(\cdot,\cdot)$
\end_inset

 is the 
\series bold
Beta function
\series default
.
\end_layout

\begin_layout Itemize
Posterior model probabilities
\begin_inset Formula 
\[
Pr(M_{k}|x_{1},...,x_{n})\propto p(x_{1},...,x_{n}|M_{k})Pr(M_{k}),\text{ for }k=0,1.
\]

\end_inset


\end_layout

\begin_layout Itemize
The Bayes factor 
\begin_inset Formula 
\[
BF(M_{0};M_{1})=\frac{p(x_{1},...,x_{n}\mathbf{|}H_{0})}{p(x_{1},...,x_{n}\mathbf{|}H_{1})}=\frac{\theta_{0}^{s}(1-\theta_{0})^{f}B(\alpha,\beta)}{B(\alpha+s,\beta+f)}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bayesian hypothesis testing - Bernoulli example
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
This is equivalent to the posterior under the following 'spike-and-slab'
 prior:
\begin_inset Formula 
\[
p(\theta)=\pi I_{\theta_{0}}(\theta)+(1-\pi)Beta(\alpha,\beta)
\]

\end_inset


\end_layout

\begin_layout Itemize
Note: data can now 
\shape italic
support
\shape default
 a null hypothesis (not only reject it).
 This is all due to the introduction of a prior.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bayesian hypothesis testing, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Bayes tests are consistent (not true for frequentist test) 
\begin_inset Formula 
\[
p(H_{k}\mathbf{|x})\rightarrow1\text{ as }n\rightarrow\infty\text{ if }H_{k}\text{ is true.}
\]

\end_inset


\end_layout

\begin_layout Itemize
The priors must be proper.
 Example: Let 
\begin_inset Formula $x_{1},...,x_{n}$
\end_inset

 be an independent sample from 
\begin_inset Formula $N(\theta,1)$
\end_inset

.
 
\begin_inset Formula 
\begin{eqnarray*}
H_{0} & : & \theta=\theta_{0}\\
H_{1} & : & \theta\neq\theta_{0}\text{, \ with prior }N(\mu_{0},\tau_{0}^{2})\text{ if }H_{1}\text{ holds.}
\end{eqnarray*}

\end_inset

Then it can be shown that:
\begin_inset Formula 
\[
p(H_{0}\mathbf{|x})\rightarrow1\text{ as }\tau_{0}^{2}\rightarrow\infty,
\]

\end_inset

regardless of which hypothesis is the true one.
 
\end_layout

\begin_layout Itemize
This result is entirely in the logic of Bayesian testing!
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example - Variable selection
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Linear regression:
\begin_inset Formula 
\[
y=\beta_{0}+\beta_{1}x_{1}+...+\beta_{p}x_{p}+\varepsilon.
\]

\end_inset

Which variables have non-zero coefficient? Example of hypotheses: 
\begin_inset Formula 
\begin{eqnarray*}
H_{0} & : & \beta_{0}=\beta_{1}=...\beta_{p}=0\\
H_{1} & : & \beta_{1}=0\\
H_{2} & : & \beta_{1}=\beta_{2}=0
\end{eqnarray*}

\end_inset

we could consider all possible subsets of 
\begin_inset Formula $\beta$
\end_inset

 coefficients to be zero.
 Easy! Just compute the marginal likelihood of each hypothesis.
\end_layout

\begin_layout Itemize
MCMC sampling algorithms for variable selection.
 Introduce variable indicators:
\begin_inset Formula 
\[
I_{j}=\begin{cases}
0 & \text{ if }\beta_{j}=0\\
1 & \text{ if }\beta_{j}\neq0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Itemize
Sample from the joint posterior 
\begin_inset Formula $p(\beta_{0},\beta_{1},...,\beta_{p},I_{1},I_{2},...,I_{p}|y,x)$
\end_inset

 using Gibbs sampling (linear Gaussian regression) or Metropolis-Hastings
 (everything else).
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example: Model choice in multivariate time series
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Multivariate time series
\begin_inset Formula 
\[
x_{t}=\alpha z_{t}+\Phi_{1}x_{t-1}+...\Phi_{k}x_{t-k}+\Psi_{1}+\Psi_{2}t+\Psi_{3}t^{2}+\varepsilon_{t}
\]

\end_inset


\end_layout

\begin_layout Itemize
Need to choose:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Lag length
\series default
, 
\begin_inset Formula $k=1,2..,4$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Trend model
\series default
 (
\begin_inset Formula $s=1,2,...,5$
\end_inset

)
\end_layout

\begin_layout Itemize
Number of 
\series bold
long-run (cointegration) relations
\series default
: 
\begin_inset Formula $r=0,1,2,3,4.$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename MonetaryDataModelPost.pdf
	scale 70

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Standard

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Model averaging
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\gamma$
\end_inset

 be a quanitity with an interpretation which stays the same across the two
 models (for example a future value of the data 
\begin_inset Formula $\tilde{y}$
\end_inset

).
 The marginal posterior distribution of 
\begin_inset Formula $\gamma$
\end_inset

 reads
\begin_inset Formula 
\[
p(\gamma|y)=p(M_{1}|y)p_{1}(\gamma|y)+p(M_{2}|y)p_{2}(\gamma|y),
\]

\end_inset

where 
\begin_inset Formula $p_{k}(\gamma|y)$
\end_inset

 is the marginal posterior of 
\begin_inset Formula $\gamma$
\end_inset

 conditional on model 
\begin_inset Formula $k$
\end_inset

.
\end_layout

\begin_layout Itemize
Prediction: 
\begin_inset Formula $\gamma=(y_{T+1},...,y_{T+h})$
\end_inset

'.
\end_layout

\begin_layout Itemize
Predictive distribution includes three sources of uncertainty:
\end_layout

\begin_deeper
\begin_layout Itemize
Future errors/disturbances (e.g.
 the 
\begin_inset Formula $\varepsilon$
\end_inset

's in a regression)
\end_layout

\begin_layout Itemize
Parameter uncertainty (the predictive distribution 
\begin_inset Formula $p(\tilde{y}|y$
\end_inset

) has the parameters integrated out by their posteriors)
\end_layout

\begin_layout Itemize
Model uncertainty (by model averaging)
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Marginal likelihood as measure of out-of-sample predictive performance 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The marginal likelihood of a sample 
\begin_inset Formula $y_{1},...,y_{T}$
\end_inset

 can be expressed as
\begin_inset Formula 
\[
p(y_{1},...,y_{n})=p(y_{1})p(y_{2}|y_{1})\cdots p(y_{n}|y_{1},y_{2},...,y_{n-1})
\]

\end_inset


\begin_inset Formula 
\[
p(y_{t}|y_{1},...,y_{t-1})=\int p(y_{t}|\theta)p(\theta|y_{1},...,y_{t-1})d\theta
\]

\end_inset

where we assume that 
\begin_inset Formula $y_{t}$
\end_inset

 is independent of 
\begin_inset Formula $y_{1},...,y_{t-1}$
\end_inset

 conditional on 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Itemize
The prediction of 
\begin_inset Formula $y_{1}$
\end_inset

 is based on the prior of 
\begin_inset Formula $\theta$
\end_inset

, and is therefore sensitive to the prior.
\end_layout

\begin_layout Itemize
The prediction of 
\begin_inset Formula $y_{T}$
\end_inset

 uses almost all the data to infer 
\begin_inset Formula $\theta$
\end_inset

.
 Very little influenced by the prior when 
\begin_inset Formula $T$
\end_inset

 is not small.
\end_layout

\begin_layout Itemize
Log Predictive Score.
\end_layout

\begin_layout Itemize
Cross-validation.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Marginal likelihood in conjugate models
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Computing the marginal likelihood requires integration w.r.t.
 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Itemize
Short cut for conjugate models by rearragement of Bayes' theorem:
\begin_inset Formula 
\[
p(y)=\frac{p(y|\theta)p(\theta)}{p(\theta|y)}
\]

\end_inset


\end_layout

\begin_layout Itemize
Bernoulli model example
\begin_inset Formula 
\begin{align*}
p(\theta) & =\frac{1}{B(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}\\
p(y|\theta) & =\theta^{s}(1-\theta)^{f}\\
p(\theta|y) & =\frac{1}{B(\alpha+s,\beta+f)}\theta^{\alpha+s-1}(1-\theta)^{\beta+f-1}
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
Marginal likelihood
\begin_inset Formula 
\[
p(y)=\frac{\theta^{s}(1-\theta)^{f}\frac{1}{B(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}}{\frac{1}{B(\alpha+s,\beta+f)}\theta^{\alpha+s-1}(1-\theta)^{\beta+f-1}}=\frac{B(\alpha+s,\beta+f)}{B(\alpha,\beta)}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Computing the marginal likelihood
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Usually difficult to evaluate the integral
\begin_inset Formula 
\[
p(y)=\int p(y|\theta)p(\theta)d\theta=E_{p(\theta)}[p(y|\theta)].
\]

\end_inset


\end_layout

\begin_layout Itemize
A (naive) first try is
\begin_inset space \space{}
\end_inset

to draw from the prior 
\begin_inset Formula $\theta^{(1)},...,\theta^{(N)}$
\end_inset

 and estimating the marginal likelihood by the average likelihood
\begin_inset Formula 
\[
\hat{p}(y)=\frac{1}{N}\sum_{i=1}^{N}p(y|\theta^{(i)}).
\]

\end_inset

Unstable if the posterior is very different from the prior.
\end_layout

\begin_layout Itemize
Importance sampling.
 Let 
\begin_inset Formula $\theta^{(1)},...,\theta^{(N)}$
\end_inset

 be iid draws from some density 
\begin_inset Formula $g(\theta)$
\end_inset

.
 
\begin_inset Formula 
\begin{align*}
p(y) & =\int p(y|\theta)p(\theta)d\theta=\int\frac{p(y|\theta)p(\theta)}{g(\theta)}g(\theta)d\theta\\
 & =\mbox{E}_{g}\left[\frac{p(y|\theta)p(\theta)}{g(\theta)}\right]\approx N^{-1}\sum_{i=1}^{N}\frac{p(y|\theta^{(i)})p(\theta^{(i)})}{g(\theta^{(i)})}.
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Computing the marginal likelihood, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Rearrangement of Bayes' theorem: 
\begin_inset Formula 
\[
p(y)=\frac{p(y|\theta)p(\theta)}{p(\theta|y)}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Problem: we must know the posterior, 
\series bold
including
\series default
 the normalization constant.
 The 
\begin_inset Formula $\propto$
\end_inset

 trick does not work here.
\end_layout

\begin_layout Itemize
But we only need to know 
\begin_inset Formula $p(\theta|y)$
\end_inset

 in a single point 
\begin_inset Formula $\theta_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
Kernel density estimator may be used to approximate 
\begin_inset Formula $p(\theta_{0}|y)$
\end_inset

.
 Unstable.
 Chib (1995, JASA) and Chib-Jeliazkov (2001, JASA) provide better solutions.
\end_layout

\begin_layout Itemize
Reversible Jump MCMC
\begin_inset space \space{}
\end_inset

(RJMCMC) for model inference.
\end_layout

\begin_deeper
\begin_layout Itemize
MCMC
\begin_inset space \space{}
\end_inset

methods can be extended to not only move in the parameter space for a given
 model, but also jumping between models.
 
\end_layout

\begin_layout Itemize
The proportion of iterations spent in model 
\begin_inset Formula $k$
\end_inset

 is an estimate of 
\begin_inset Formula $\Pr(M_{k}|y)$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Approximate marginal likelihoods
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Taylor approximation of the log posterior
\begin_inset Formula 
\begin{align*}
\ln p(\mathbf{y}|\theta)p(\theta) & \approx\ln p(\mathbf{y}|\hat{\theta})+\ln p(\hat{\theta})-\frac{1}{2}J_{\hat{\theta},\mathbf{y}}(\theta-\hat{\theta})^{2},
\end{align*}

\end_inset


\begin_inset Formula 
\[
p(\mathbf{y}|\theta)p(\theta)\approx p(\mathbf{y}|\hat{\theta})p(\hat{\theta})\exp\left[-\frac{1}{2}J_{\hat{\theta},\mathbf{y}}(\theta-\hat{\theta})^{2}\right],
\]

\end_inset

which can be integrated analytically w.r.t.
 
\begin_inset Formula $\theta$
\end_inset

, using properties of the multivariate normal pdf.
\end_layout

\begin_layout Itemize
The Laplace approximation: 
\begin_inset Formula 
\[
\ln\hat{p}(\mathbf{y})=\ln p(\mathbf{y}|\hat{\theta})+\text{ln}p(\hat{\theta})+\frac{1}{2}\ln\left\vert J_{\hat{\theta},\mathbf{y}}^{-1}\right\vert +\frac{p}{2}\ln(2\pi),
\]

\end_inset

where 
\begin_inset Formula $p$
\end_inset

 is the number of unrestricted parameters in the model.
\end_layout

\begin_layout Itemize
Cruder version of the Laplace: The SBC (BIC) approximation
\begin_inset Formula 
\[
\ln\hat{p}(\mathbf{y})=\ln p(\mathbf{y}|\hat{\theta})+\ln p(\hat{\theta})-\frac{p}{2}\ln n.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bayesian model inference - a critique
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Bayes factors (model probabilities) are very sharp inference objects.
 Handle with care.
\end_layout

\begin_layout Itemize
Minor differences in the prior can lead to large differences in the Bayes
 factor, especially in high-dimensional non-linear models.
\end_layout

\begin_layout Itemize
Continuous model expansion is usually a better alternative, when feasible.
\end_layout

\begin_layout Itemize
Improper priors cannot be used to compute Bayes factors.
 Several tricks have been developed to handle this, but they are non-Bayesian.
\end_layout

\begin_layout Itemize
Bayes factors are relative measures, all models under consideration may
 be bad approximations to the data.
\end_layout

\begin_layout Itemize
Bayes model probabilities essentially assume the true data generating process
 (DGP) is among the compared models.
 Box: All models are false, but some are useful.
\end_layout

\begin_layout Itemize
When none of the compared models are true: 
\begin_inset Formula $Pr(M_{i}|y)\rightarrow1$
\end_inset

 for the model which is closest to the DGP in the Kullback-Leibler sense.
 Putting all eggs in one basket is not always a good idea.'
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Models - why?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We now know how to 
\series bold
compare
\series default
 models.
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
But how do we know if any given model is 'any good'?
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
George Box: '
\series bold
All models are false, but some are useful
\series default
'.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
What is your model for really?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Prediction
\series default
.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Interpretation not a concern
\end_layout

\begin_layout Itemize
Black-box approach may be ok.
 
\end_layout

\begin_layout Itemize
Extrapolation? 
\end_layout

\begin_layout Itemize
Model averaging may be a good idea.
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Abstraction to 
\series bold
aid in thinking
\series default
 about a phenomena.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Prediction accuracy of less concern.
 
\end_layout

\begin_layout Itemize
Model averaging may be a bad idea.
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Model as a 
\series bold
compact description of a complex phenomena
\series default
.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Computational cost of model evaluation may be a concern.
 
\end_layout

\begin_layout Itemize
Online/real-time analysis.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Posterior predictive analysis
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $p(y|\theta)$
\end_inset

 is a 'good' model, then the data actually observed should not differ 'too
 much' from simulated data from 
\begin_inset Formula $p(y|\theta)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Bayesian: simulate data from the 
\series bold
posterior predictive distribution
\series default
:
\begin_inset Formula 
\[
p(y^{rep}|y)=\int p(y^{rep}|\theta)p(\theta|y)d\theta.
\]

\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Difficult to compare 
\begin_inset Formula $y$
\end_inset

 and 
\begin_inset Formula $y^{rep}$
\end_inset

 because of dimensionality.
 
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Solution: compare 
\series bold
low-dimensional
\series default
 
\series bold
statistic
\series default
 
\begin_inset Formula $T(y,\theta)$
\end_inset

 to 
\begin_inset Formula $T(y^{rep},\theta)$
\end_inset

.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Evaluates the full probability model consisting of both the likelihood 
\shape italic
and
\shape default
 prior distribution.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Posterior predictive analysis, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Algorithm
\series default
 for simulating from the posterior predictive density 
\begin_inset Formula $p[T(y^{rep})|y]$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

[
\end_layout

\end_inset

1
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

]
\end_layout

\end_inset

 Draw a 
\begin_inset Formula $\theta^{(1)}$
\end_inset

 from the posterior 
\begin_inset Formula $p(\theta|y)$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

[
\end_layout

\end_inset

2
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

]
\end_layout

\end_inset

 Simulate a data-replicate 
\begin_inset Formula $y^{(1)}$
\end_inset

 from 
\begin_inset Formula $p(y^{rep}|\theta^{(1)})$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

[
\end_layout

\end_inset

3
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

]
\end_layout

\end_inset

 Compute 
\begin_inset Formula $T(y^{(1)})$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

[
\end_layout

\end_inset

4
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

]
\end_layout

\end_inset

 Repeat steps 1-3 a large number of times to obtain a sample from 
\begin_inset Formula $T(y^{rep})$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We may now compare the observed statistic 
\begin_inset Formula $T(y)$
\end_inset

 with the distribution of 
\begin_inset Formula $T(y^{rep})$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Posterior predictive p-value
\series default
: 
\begin_inset Formula $\Pr[T(y^{rep})\geq T(y)]$
\end_inset

 
\end_layout

\begin_layout Itemize
Informal 
\series bold
graphical analysis
\series default
.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Posterior predictive analysis - Examples
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Ex.
 1.
 Model: 
\begin_inset Formula $y_{1},...,y_{n}\overset{iid}{\sim}N(\mu,\sigma^{2})$
\end_inset

.
 
\begin_inset Formula $T(y)=\max_{i}\left|y_{i}\right|$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Ex.
 2.
 Assumption of no reciprocity in networks.
 
\begin_inset Newline newline
\end_inset


\begin_inset Formula $y_{ij}|\theta\overset{iid}{\sim}Bernoulli(\theta)$
\end_inset

.
 
\begin_inset Formula $T(y)=$
\end_inset

proportion of reciprocated node pairs.
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Ex.
 3.
 ARIMA-process.
 
\begin_inset Formula $T(y)$
\end_inset

 may be the autocorrelation function.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Ex.
 4.
 Poisson regression.
 
\begin_inset Formula $T(y)$
\end_inset

 frequency distribution of the response counts.
 Proportions of zero counts.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Posterior predictive analysis - Normal model, max statistic
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/matvi05/Dropbox/Teaching/StatMetoder2010/F11/PostPredNormalMaxStatistic.eps
	scale 45

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The fit of a mixture of Poisson regressions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename mdvisitPlotFreqPois.eps
	scale 50

\end_inset


\end_layout

\end_deeper
\end_body
\end_document
