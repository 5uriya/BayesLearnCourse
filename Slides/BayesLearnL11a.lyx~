#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass beamer
\begin_preamble
\setcounter{MaxMatrixCols}{10}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathpazo}
\usepackage{hyperref}
\usepackage{multimedia}
\usepackage{xcolor}
\usepackage{colortbl}
\definecolor{RawSienna}{cmyk}{0,0.87,0.82,0.31}
\definecolor{gray97}{cmyk}{0,0,0,0.03}
\definecolor{robinsegg}{cmyk}{0.18,0.04,0,0.07}
\definecolor{cola}{cmyk}{0,0.315,0.35,0.155}

\newenvironment{stepenumerate}{\begin{enumerate}[<+->]}{\end{enumerate}}
\newenvironment{stepitemize}{\begin{itemize}[<+->]}{\end{itemize} }
\newenvironment{stepenumeratewithalert}{\begin{enumerate}[<+-| alert@+>]}{\end{enumerate}}
\newenvironment{stepitemizewithalert}{\begin{itemize}[<+-| alert@+>]}{\end{itemize} }
\usecolortheme[named=RawSienna]{structure}
%\usecolortheme[RGB={205,0,0}]{structure}
\setbeamertemplate{navigation symbols}{}
\useoutertheme{infolines}
\usetheme{default}
\setbeamertemplate{blocks}[shadow=true]
%\setbeamerfont{structure}{shape=\itshape}
\usefonttheme{structuresmallcapsserif}
\setbeamertemplate{background canvas}{
 % \ifnum \thepage>0 \relax % we are on the first page
%\includegraphics[width=\paperwidth,height=\paperheight]{/home/mv/Dropbox/Foton/IconsWallpaper/greyribbonLighter.jpg}
 % \else
 	% No background for page 2 and onwards
 % \fi
}
\end_preamble
\options xcolor=svgnames
\use_default_options false
\begin_modules
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 0
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset Argument
status open

\begin_layout Plain Layout
Bayesian Learning
\end_layout

\end_inset

Bayesian Learning - Lecture 1
\end_layout

\begin_layout Author
\begin_inset Argument
status open

\begin_layout Plain Layout
Mattias Villani
\end_layout

\end_inset

Mattias Villani
\end_layout

\begin_layout Institute

\series bold
\begin_inset Argument
status open

\begin_layout Plain Layout

\series bold
Statistics, LiU
\end_layout

\end_inset

Division of Statistics
\begin_inset Newline newline
\end_inset

Department of Computer and Information Science
\begin_inset Newline newline
\end_inset

Link√∂ping University 
\end_layout

\begin_layout Date
\begin_inset space \thinspace{}
\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Model comparison - The likelihood is not useful
\end_layout

\begin_layout Itemize
Consider two models: 
\begin_inset Formula $M_{1}$
\end_inset

 and 
\begin_inset Formula $M_{2}$
\end_inset

.
 Let :
\begin_inset Formula $p_{i}(y|\theta_{i})$
\end_inset

 denote the data density under model 
\begin_inset Formula $M_{i}$
\end_inset

.
 If we knew the values of 
\begin_inset Formula $\theta_{1}$
\end_inset

 and 
\begin_inset Formula $\theta_{2}$
\end_inset

, then the likelihood ratio 
\begin_inset Formula 
\[
\frac{p_{1}(y|\theta_{1})}{p_{2}(y|\theta_{2})},
\]

\end_inset

could be used to compare the models.
\end_layout

\begin_layout Itemize
What if the model parameters are unknown? The estimated likelihood ratio:
\begin_inset Formula 
\[
\frac{p_{1}(y|\hat{\theta}_{1})}{p_{2}(y|\hat{\theta}_{2})}.
\]

\end_inset

where 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

 are the maximum likelihood estimates.
\end_layout

\begin_layout Itemize
Estimated likelihood ratio is useless in itself as the larger model always
 has larger likelihood.
 Comparison with sampling distribution of the estimated likelihood ratio
 is one solution.
\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Enter Bayes
\end_layout

\begin_layout Itemize
Bayesian: use your priors 
\begin_inset Formula $p_{1}(\theta_{1})$
\end_inset

 och 
\begin_inset Formula $p_{2}(\theta_{2})$
\end_inset

 and compute the 
\series bold
marginal likelihood
\series default
, or 
\series bold
prior predictive density
\series default
, for each model
\begin_inset Formula 
\[
p_{i}(y)=\int p_{i}(y|\theta_{i})p_{i}(\theta_{i})d\theta_{i}.
\]

\end_inset


\end_layout

\begin_layout Itemize
The Bayes factor can be used to compare to models
\begin_inset Formula 
\[
B_{12}(y)=\frac{p_{1}(y)}{p_{2}(y)}.
\]

\end_inset


\end_layout

\begin_layout Itemize
The marginal likelihoods may be converted into posterior probabilities of
 the models (
\begin_inset Formula $M_{1},M_{2}$
\end_inset

):
\begin_inset Formula 
\[
\frac{p(M_{1}|y)}{p(M_{2}|y)}=\frac{p(M_{1})}{p(M_{2})}B_{12}(y),
\]

\end_inset

where 
\begin_inset Formula $B_{12}(y)$
\end_inset

 is the Bayes factor in favor of 
\begin_inset Formula $M_{1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\text{Posterior model odds ratio = Prior model odds ratio }\cdot\text{Bayes factor}
\]

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Bayesian hypothesis testing - Bernoulli example
\end_layout

\begin_layout Itemize
Hypothesis testing is just a special case of model selection: 
\begin_inset Formula 
\begin{align*}
M_{0}: & x_{1},...,x_{n}\overset{iid}{\sim}Bernoulli(\theta_{0})\\
M_{1}: & x_{1},...,x_{n}\overset{iid}{\sim}Bernoulli(\theta),\text{}\theta\sim Beta(\alpha,\beta)
\end{align*}

\end_inset


\begin_inset Formula 
\[
p(x_{1},...,x_{n}|M_{0})=\theta_{0}^{y}(1-\theta_{0})^{n-y},
\]

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
p(x_{1},...,x_{n}|M_{1}) & = & \int_{0}^{1}\theta^{y}(1-\theta)^{n-y}B(\alpha,\beta)^{-1}\theta^{\alpha-1}(1-\theta)^{\beta-1}d\theta\\
 & = & B(y+\alpha,n-y+\beta)/B(\alpha,\beta),
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $y=\sum_{i=1}^{n}x_{i}$
\end_inset

, and 
\begin_inset Formula $B(\cdot,\cdot)$
\end_inset

 is the Beta function.
\end_layout

\begin_layout Itemize
Posterior model probabilities
\begin_inset Formula 
\[
Pr(M_{i}|x_{1},...,x_{n})\propto p(x_{1},...,x_{n}|M_{i})Pr(M_{i}),\text{ }i=0,1.
\]

\end_inset


\end_layout

\begin_layout Itemize
The Bayes factor 
\begin_inset Formula 
\[
BF(M_{0};M_{1})=\frac{p(x_{1},...,x_{n}\mathbf{|}H_{0})}{p(x_{1},...,x_{n}\mathbf{|}H_{1})}=\frac{\theta_{0}^{y}(1-\theta_{0})^{n-y}B(\alpha,\beta)}{B(y+\alpha,n-y+\beta)}.
\]

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Bayesian hypothesis testing - Bernoulli example
\end_layout

\begin_layout Itemize
This is equivalent to the posterior under the following 'spike-and-slab'
 prior:
\begin_inset Formula 
\[
p(\theta)=\pi I_{\theta_{0}}(\theta)+(1-\pi)Beta(\alpha,\beta)
\]

\end_inset


\end_layout

\begin_layout Itemize
Note: data can now 
\shape italic
support
\shape default
 a null hypothesis (not only reject it).
 This is all due to the introduction of a prior.
\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Bayesian hypothesis testing, cont.
\end_layout

\begin_layout Itemize
Bayes tests are consistent (not true for frequentist test) 
\begin_inset Formula 
\[
p(H_{i}\mathbf{|x})\rightarrow1\text{ as }n\rightarrow\infty\text{ if }H_{i}\text{ is true.}
\]

\end_inset


\end_layout

\begin_layout Itemize
The priors must be proper.
 Example: Let 
\begin_inset Formula $x_{1},...,x_{n}$
\end_inset

 be an independent sample from 
\begin_inset Formula $N(\theta,1)$
\end_inset

.
 
\begin_inset Formula 
\begin{eqnarray*}
H_{0} & : & \theta=\theta_{0}\\
H_{1} & : & \theta\neq\theta_{0}\text{, \ with prior }N(\theta_{0},\tau^{2})\text{ if }H_{1}\text{ holds.}
\end{eqnarray*}

\end_inset

Then it can be shown that:
\begin_inset Formula 
\[
p(H_{0}\mathbf{|x})\rightarrow1\text{ as }\tau^{2}\rightarrow\infty,
\]

\end_inset

regardless of which hypothesis is the true one.
 
\end_layout

\begin_layout Itemize
This result is entirely in the logic of Bayesian testing!
\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Example - Variable selection
\end_layout

\begin_layout Itemize
Linear regression:
\begin_inset Formula 
\[
y=\beta_{0}+\beta_{1}x_{1}+...+\beta_{p}x_{p}+\varepsilon.
\]

\end_inset

Which variables have non-zero coefficient? Example of hypotheses: 
\begin_inset Formula 
\begin{eqnarray*}
H_{0} & : & \beta_{0}=\beta_{1}=...\beta_{p}=0\\
H_{1} & : & \beta_{1}=0\\
H_{2} & : & \beta_{1}=\beta_{2}=0
\end{eqnarray*}

\end_inset

we could consider all possible subsets of 
\begin_inset Formula $\beta$
\end_inset

 coefficients to be zero.
 Easy! Just compute the marginal likelihood of each hypothesis.
\end_layout

\begin_layout Itemize
MCMC sampling algorithms for variable selection.
 Introduce variable indicators:
\begin_inset Formula 
\[
I_{j}=\begin{cases}
0 & \text{ if }\beta_{j}=0\\
1 & \text{ if }\beta_{j}\neq0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Itemize
Sample from the joint posterior 
\begin_inset Formula $p(\beta_{0},\beta_{1},...,\beta_{p},I_{1},I_{2},...,I_{p}|y,x)$
\end_inset

 using Gibbs sampling (linear Gaussian regression) or Metropolis-Hastings
 (everything else).
\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Model averaging
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\gamma$
\end_inset

 be a quanitity with an interpretation which stays the same across the two
 models (for example a future value of the data 
\begin_inset Formula $\tilde{y}$
\end_inset

).
 The marginal posterior distribution of 
\begin_inset Formula $\gamma$
\end_inset

 reads
\begin_inset Formula 
\[
p(\gamma|y)=p(M_{1}|y)p_{1}(\gamma|y)+p(M_{2}|y)p_{2}(\gamma|y),
\]

\end_inset

where 
\begin_inset Formula $p_{i}(\gamma|y)$
\end_inset

 is the marginal posterior of 
\begin_inset Formula $\gamma$
\end_inset

 conditional on model 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Itemize
Prediction: 
\begin_inset Formula $\gamma=(y_{T+1},...,y_{T+h})$
\end_inset

'.
\end_layout

\begin_layout Itemize
Predictive distribution includes three sources of uncertainty:
\end_layout

\begin_deeper
\begin_layout Itemize
Future errors/disturbances (e.g.
 the 
\begin_inset Formula $\varepsilon$
\end_inset

's in a regression)
\end_layout

\begin_layout Itemize
Parameter uncertainty (the predictive distribution 
\begin_inset Formula $p(y_{future}|y_{known}$
\end_inset

) has the parameters integrated out by their posteriors)
\end_layout

\begin_layout Itemize
Model uncertainty (by model averaging)
\end_layout

\end_deeper
\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Marginal likelihood as measure of out-of-sample predictive performance 
\end_layout

\begin_layout Itemize
The marginal likelihood of a sample 
\begin_inset Formula $y_{1},...,y_{T}$
\end_inset

 can be expressed as
\begin_inset Formula 
\[
p(y_{1},...,y_{n})=p(y_{1})p(y_{2}|y_{1})\cdots p(y_{n}|y_{1},y_{2},...,y_{n-1})
\]

\end_inset


\begin_inset Formula 
\[
p(y_{t}|y_{1},...,y_{t-1})=\int p(y_{t}|\theta)p(\theta|y_{1},...,y_{t-1})d\theta
\]

\end_inset

where we assume that 
\begin_inset Formula $y_{t}$
\end_inset

 is independent of 
\begin_inset Formula $y_{1},...,y_{t-1}$
\end_inset

 conditional on 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Itemize
The prediction of 
\begin_inset Formula $y_{1}$
\end_inset

 is based on the prior of 
\begin_inset Formula $\theta$
\end_inset

, are is therefore sensitive to the prior.
\end_layout

\begin_layout Itemize
The prediction of 
\begin_inset Formula $y_{T}$
\end_inset

 uses almost all the data to infer 
\begin_inset Formula $\theta$
\end_inset

.
 Very little influenced by the prior when 
\begin_inset Formula $T$
\end_inset

 is not small.
\end_layout

\begin_layout Itemize
Log Predictive Score.
\end_layout

\begin_layout Itemize
Cross-validation.
\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Computing the marginal likelihood in conjugate models
\end_layout

\begin_layout Itemize
Computing the marginal likelihood requires integration
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula 
\[
p(y)=\int p(y|\theta)p(\theta)d\theta
\]

\end_inset


\end_layout

\begin_layout Itemize
Rearragement of Bayes' theorem gives the marginal likelihood
\begin_inset Formula 
\[
p(y)=\frac{p(y|\theta)p(\theta)}{p(\theta|y)}
\]

\end_inset


\end_layout

\begin_layout Itemize
Bernoulli model again
\begin_inset Formula 
\begin{align*}
p(\theta) & =\frac{1}{B(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}\\
p(y|\theta) & =\theta^{s}(1-\theta)^{f}\\
p(\theta|y) & =\frac{1}{B(\alpha+s,\beta+f)}\theta^{\alpha+s-1}(1-\theta)^{\beta+f-1}
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
Marginal likelihood
\begin_inset Formula 
\[
p(y)=\frac{p(y|\theta)p(\theta)}{p(\theta|y)}=\frac{\theta^{s}(1-\theta)^{f}\frac{1}{B(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}}{\frac{1}{B(\alpha+s,\beta+f)}\theta^{\alpha+s-1}(1-\theta)^{\beta+f-1}}=\frac{B(\alpha+s,\beta+f)}{B(\alpha,\beta)}
\]

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Computing the marginal likelihood
\end_layout

\begin_layout Itemize
Usually difficult to evaluate the integral
\begin_inset Formula 
\[
p(y)=\int p(y|\theta)p(\theta)d\theta=E_{p(\theta)}[p(y|\theta)].
\]

\end_inset


\end_layout

\begin_layout Itemize
A (naive) first try is
\begin_inset space \space{}
\end_inset

to draw from the prior 
\begin_inset Formula $\theta^{(1)},...,\theta^{(N)}$
\end_inset

 and estimating the marginal likelihood by the average likelihood
\begin_inset Formula 
\[
\hat{p}(y)=\frac{1}{N}\sum_{i=1}^{N}p(y|\theta^{(i)}).
\]

\end_inset

Unstable if the posterior is very different from the prior.
\end_layout

\begin_layout Itemize
Importance sampling.
 Let 
\begin_inset Formula $\theta^{(1)},...,\theta^{(N)}$
\end_inset

 be iid draws from some density 
\begin_inset Formula $g(\theta)$
\end_inset

.
 
\begin_inset Formula 
\begin{align*}
p(y) & =\int p(y|\theta)p(\theta)d\theta=\int\frac{p(y|\theta)p(\theta)}{g(\theta)}g(\theta)d\theta\\
 & =\mbox{E}_{g}\left[\frac{p(y|\theta)p(\theta)}{g(\theta)}\right]\approx N^{-1}\sum_{i=1}^{N}\frac{p(y|\theta^{(i)})p(\theta^{(i)})}{g(\theta^{(i)})}.
\end{align*}

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Computing the marginal likelihood, cont.
\end_layout

\begin_layout Itemize
Bayes theorem may be rearranged to give the identity:
\begin_inset Formula 
\[
p(y)=\frac{p(y|\theta)p(\theta)}{p(\theta|y)}.
\]

\end_inset

Problem: need to know 
\begin_inset Formula $p(\theta|y)$
\end_inset

 in a single point 
\begin_inset Formula $\theta_{0}$
\end_inset

 (NOTE: including normalization constant!).
 Kernel density estimator may be used to approximate 
\begin_inset Formula $p(\theta_{0}|y)$
\end_inset

.
 Unstable.
 Chib (1995, JASA) and Chib-Jeliazkov (2001, JASA) provide solutions.
\end_layout

\begin_layout Itemize
MCMC
\begin_inset space \space{}
\end_inset

methods can be extended to not only move in the parameter space for a given
 model, but also jumping between models.
 The proportion of iterations spent in model 
\begin_inset Formula $i$
\end_inset

 is an estimate of 
\begin_inset Formula $\Pr(M_{i}|y)$
\end_inset

.
 Reversible Jump MCMC
\begin_inset space \space{}
\end_inset

(RJMCMC).
\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Computing marginal likelihoods by approximation
\end_layout

\begin_layout Itemize
Taylor approximation of the log posterior
\begin_inset Formula 
\begin{align*}
\ln p(y|\theta)p(\theta) & \approx\ln p(y|\hat{\theta})+\ln p(\hat{\theta})-\frac{1}{2}H_{y}(\hat{\theta})(\theta-\hat{\theta})^{2},
\end{align*}

\end_inset


\begin_inset Formula 
\[
H_{y}(\hat{\theta})=-\frac{\partial^{2}\ln p(\theta|y)}{\partial\theta^{2}}|_{\theta=\hat{\theta}}
\]

\end_inset


\begin_inset Formula 
\[
p(y|\theta)p(\theta)\approx p(y|\hat{\theta})p(\hat{\theta})\exp\left[-\frac{1}{2}H_{y}(\hat{\theta})(\theta-\hat{\theta})^{2}\right],
\]

\end_inset

which can be integrated analytically using properties of the multivariate
 normal pdf.
\end_layout

\begin_layout Itemize
The Laplace approximation: 
\begin_inset Formula 
\[
\ln\hat{p}(y)=\ln p(y|\hat{\theta})+\text{ln}p(\hat{\theta})+\frac{1}{2}\ln\left\vert H_{y}^{-1}(\hat{\theta})\right\vert +\frac{k}{2}\ln(2\pi),
\]

\end_inset

where 
\begin_inset Formula $k$
\end_inset

 is the number of unrestricted parameters in the model.
\end_layout

\begin_layout Itemize
Cruder version of the Laplace: The SBC (BIC) approximation
\begin_inset Formula 
\[
\ln\hat{p}(y)=\ln p(y|\hat{\theta})+\ln p(\hat{\theta})-\frac{k}{2}\ln n.
\]

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Bayesian model inference - a critique
\end_layout

\begin_layout Itemize
Bayes factors (model probabilities) are very sharp inference objects.
 Handle with care.
\end_layout

\begin_layout Itemize
Minor differences in the prior can lead to large differences in the Bayes
 factor, especially in high-dimensional non-linear models.
\end_layout

\begin_layout Itemize
Continuous model expansion is usually a better alternative, when feasible.
\end_layout

\begin_layout Itemize
Improper priors cannot be used to compute Bayes factors.
 Several tricks have been developed to handle this, but they are non-Bayesian.
\end_layout

\begin_layout Itemize
Bayes factors are relative measures, all models under consideration may
 be bad approximations to the data.
\end_layout

\begin_layout Itemize
Bayes model probabilities essentially assume the true data generating process
 (DGP) is among the compared models.
 Box: All models are false, but some are useful.
\end_layout

\begin_layout Itemize
When none of the compared models are true: 
\begin_inset Formula $Pr(M_{i}|y)\rightarrow1$
\end_inset

 for the model which is closest to the DGP in the Kullback-Leibler sense.
 Putting all eggs in one basket is not always a good idea.
\end_layout

\begin_layout EndFrame

\end_layout

\end_body
\end_document
