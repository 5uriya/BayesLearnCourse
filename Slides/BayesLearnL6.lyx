#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass beamer
\begin_preamble
\setcounter{MaxMatrixCols}{10}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathpazo}
\usepackage{hyperref}
\usepackage{multimedia}
\usepackage{xcolor}
\usepackage{colortbl}
\definecolor{RawSienna}{cmyk}{0,0.87,0.82,0.31}
\definecolor{gray97}{cmyk}{0,0,0,0.03}
\definecolor{robinsegg}{cmyk}{0.18,0.04,0,0.07}
\definecolor{cola}{cmyk}{0,0.315,0.35,0.155}

\newenvironment{stepenumerate}{\begin{enumerate}[<+->]}{\end{enumerate}}
\newenvironment{stepitemize}{\begin{itemize}[<+->]}{\end{itemize} }
\newenvironment{stepenumeratewithalert}{\begin{enumerate}[<+-| alert@+>]}{\end{enumerate}}
\newenvironment{stepitemizewithalert}{\begin{itemize}[<+-| alert@+>]}{\end{itemize} }
\usecolortheme[named=RawSienna]{structure}
%\usecolortheme[RGB={205,0,0}]{structure}
\setbeamertemplate{navigation symbols}{}
\useoutertheme{infolines}
\usetheme{default}
\setbeamertemplate{blocks}[shadow=true]
%\setbeamerfont{structure}{shape=\itshape}
\usefonttheme{structuresmallcapsserif}
\setbeamertemplate{background canvas}{
 % \ifnum \thepage>0 \relax % we are on the first page
%\includegraphics[width=\paperwidth,height=\paperheight]{/home/mv/Dropbox/Foton/IconsWallpaper/greyribbonLighter.jpg}
 % \else
 	% No background for page 2 and onwards
 % \fi
}
\end_preamble
\options xcolor=svgnames
\use_default_options false
\begin_modules
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 0
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset Argument
status open

\begin_layout Plain Layout
Bayesian Learning
\end_layout

\end_inset

Bayesian Learning - Lecture 6
\end_layout

\begin_layout Author
\begin_inset Argument
status open

\begin_layout Plain Layout
Mattias Villani
\end_layout

\end_inset

Mattias Villani
\end_layout

\begin_layout Institute

\series bold
\begin_inset Argument
status open

\begin_layout Plain Layout

\series bold
Statistics, LiU
\end_layout

\end_inset

Division of Statistics
\begin_inset Newline newline
\end_inset

Department of Computer and Information Science
\begin_inset Newline newline
\end_inset

Link√∂ping University 
\end_layout

\begin_layout Date
\begin_inset space \thinspace{}
\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Lecture overview
\end_layout

\begin_layout Itemize
Flexible nonlinear regression and splines
\end_layout

\begin_layout Itemize
Smoothness/shrinkage priors
\end_layout

\begin_layout Itemize
Bayesian variable selection
\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Non-parametric/Non-linear regression
\end_layout

\begin_layout Itemize
Recall the linear regression model with a single covariate
\begin_inset Formula 
\[
y_{i}=\beta_{0}+\beta_{1}x_{i}+\varepsilon_{i}\text{, \ \ \ \ \ }\varepsilon_{i}\overset{iid}{\sim}N(0,\sigma^{2}).
\]

\end_inset


\end_layout

\begin_layout Itemize
Extension to non-linearity: 
\begin_inset Formula 
\[
y_{i}=f(x_{i})+\varepsilon_{i}\text{, \ \ \ \ \ }\varepsilon_{i}\overset{iid}{\sim}N(0,\sigma^{2}),
\]

\end_inset

where 
\begin_inset Formula $f(\cdot)$
\end_inset

 is a non-linear function.
\end_layout

\begin_layout Itemize
Polynomial regression:
\begin_inset Formula 
\[
f(x_{i})=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+...+\beta_{k}x_{i}^{k}.
\]

\end_inset

This can be written as a linear regression
\begin_inset Formula 
\[
y=X_{P}\beta+\varepsilon,
\]

\end_inset

where 
\begin_inset Formula 
\[
X_{P}=(1,x,x^{2},...,x^{k}).
\]

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Polynomial basis functions
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename /home/mv/Dropbox/Teaching/BayesStat2011/Lectures/quadraticBasis.eps
	scale 55

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Smooth interpolation
\end_layout

\begin_layout Itemize
Another approach treats all 
\begin_inset Formula $n$
\end_inset

 ordinates as unknown parameters: 
\begin_inset Formula 
\[
f(x_{i})=\gamma_{i}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Problem: too many parameters.
 Estimated curve wiggles way too much.
\end_layout

\begin_layout Itemize
Solution: use a (multivariate) prior on 
\begin_inset Formula $\gamma=(\gamma_{1},...,\gamma_{n})^{\prime}$
\end_inset

 that carries the info that the regression curve is smooth:
\begin_inset Formula 
\[
\text{if }x_{i}\text{ and }x_{k}\text{ are close then }\gamma_{i}\text{ is }\text{close to }\gamma_{k}
\]

\end_inset


\end_layout

\begin_layout Itemize
Order the data with respect to covariates and assign the prior 
\begin_inset Formula 
\[
p(\gamma_{i}|\gamma_{i-1})\sim N\left(\gamma_{i-1},\tau_{0}^{2}\cdot\left|x_{i}-x_{i-1}\right|\right),\text{ for }i=2,...,n.
\]

\end_inset


\end_layout

\begin_layout Itemize
The hyperparameter 
\begin_inset Formula $\tau_{0}^{2}$
\end_inset

 controls the degree of prior smoothness.
\end_layout

\begin_layout Itemize
Gaussian process.
\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Splines
\end_layout

\begin_layout Itemize
Use 
\begin_inset Formula $m$
\end_inset

 change-points (knots) 
\begin_inset Formula $k_{1}<k_{2}<...<k_{m}$
\end_inset

.
 Construct a 'dummy variable' for each change-point:
\begin_inset Formula 
\[
b_{ij}=\left\{ \begin{array}{c}
1\text{ \ \ if }x_{i}>k_{j}\\
0\text{ otherwise}
\end{array}\right.
\]

\end_inset

Not smooth, the regression line has sudden jumps.
\end_layout

\begin_layout Itemize
Smoother: trunctated linear splines 
\begin_inset Formula 
\[
b_{ij}=\left\{ \begin{array}{c}
x_{i}-k_{j}\text{ \ \ if }x_{i}>k_{j}\\
0\text{ otherwise}
\end{array}\right.
\]

\end_inset


\end_layout

\begin_layout Itemize
Generalization: 
\shape italic
truncated power splines
\shape default

\begin_inset Formula 
\[
b_{ij}=\left\{ \begin{array}{c}
(x_{i}-k_{j})^{p}\text{ \ \ if }x_{i}>k_{j}\\
0\text{ otherwise}
\end{array}\right.
\]

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Truncated polynomial basis functions
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename /home/mv/Dropbox/Teaching/BayesStat2011/Lectures/brokenStickBasis.eps
	scale 55

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Splines, cont.
\end_layout

\begin_layout Itemize
Note: given the knots, the non-parametric spline regression model is a linear
 regression of 
\begin_inset Formula $y$
\end_inset

 on the 
\begin_inset Formula $m$
\end_inset

 'dummy variables' 
\begin_inset Formula $b_{j}$
\end_inset


\begin_inset Formula 
\[
y=X_{b}\beta+\varepsilon,
\]

\end_inset

where 
\begin_inset Formula $X_{b}$
\end_inset

 is the basis regression matrix 
\begin_inset Formula 
\[
X_{b}=(b_{1},...,b_{m}).
\]

\end_inset


\end_layout

\begin_layout Itemize
It is also common to include an intercept and the linear part of the model
 separately.
 In this case we have
\begin_inset Formula 
\[
X_{b}=(1,x,b_{1},...,b_{m}).
\]

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Smoothness prior for splines
\end_layout

\begin_layout Itemize
Problem: too many knots leads to 
\series bold
over-fitting
\series default
.
 
\end_layout

\begin_layout Itemize
Solution: 
\series bold
smoothness/shrinkage/regularization prior
\series default

\begin_inset Formula 
\[
\beta_{i}|\sigma^{2}\overset{iid}{\sim}N\left(0,\frac{\sigma^{2}}{\lambda}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
Larger 
\begin_inset Formula $\lambda$
\end_inset

 gives smoother fit.
 Note: here we have 
\begin_inset Formula $\mbox{\Omega}_{0}=\lambda I$
\end_inset

.
\end_layout

\begin_layout Itemize
Equivalent to a penalized likelihood: 
\begin_inset Formula 
\[
-2\cdot LogPost\propto RSS(\beta)+\lambda\beta'\beta
\]

\end_inset


\end_layout

\begin_layout Itemize
Posterior mean gives 
\series bold
ridge regression
\series default
 estimator
\begin_inset Formula 
\[
\tilde{\beta}=\left(X'X+\lambda I\right)^{-1}X'y
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Shrinkage
\series default
 toward zero 
\begin_inset Formula 
\[
\text{As }\lambda\rightarrow\infty,\text{ }\tilde{\beta}\rightarrow0
\]

\end_inset


\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $X'X=I$
\end_inset

 
\begin_inset Formula 
\[
\tilde{\beta}=\frac{1}{1+\lambda}\hat{\beta}_{OLS}
\]

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Bayesian spline with smoothness prior
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename /home/mv/Dropbox/Teaching/BayesStat2011/Lectures/LidarPoly1Knots24.eps
	scale 55

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Smoothness prior for splines, cont.
\end_layout

\begin_layout Itemize
The famous 
\series bold
Lasso
\series default
 variable selection method is equivalent to using the posterior mode estimate
 under the prior:
\begin_inset Formula 
\[
\beta_{i}|\sigma^{2}\overset{iid}{\sim}\mathrm{Laplace}\left(0,\frac{\sigma^{2}}{\lambda}\right)
\]

\end_inset

where the general Laplace density is
\begin_inset Formula 
\[
p(\beta_{i})=\frac{\text{1}}{2b}\exp\left(-\frac{\left|\beta_{i}-\mu\right|}{b}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
The Bayesian shrinkage prior is 
\series bold
interpretable
\series default
, and the regularization is 
\series bold
not ad hoc
\series default
.
\end_layout

\begin_layout Itemize
Laplace distribution have heavy tails.
\end_layout

\begin_layout Itemize
Laplace prior: we believe in many 
\begin_inset Formula $\beta_{i}$
\end_inset

 close to zero, but some 
\begin_inset Formula $\beta_{i}$
\end_inset

 may be very large.
\end_layout

\begin_layout Itemize
Normal distribution have light tails.
 
\end_layout

\begin_layout Itemize
Normal prior: most 
\begin_inset Formula $\beta_{i}$
\end_inset

 are fairly equal in size, and no single 
\begin_inset Formula $\beta_{i}$
\end_inset

 can be very much larger than the other ones.
\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Estimating the shrinkage
\end_layout

\begin_layout Itemize
How do we determine the degree of smoothness, 
\begin_inset Formula $\lambda$
\end_inset

? Cross-validation is one possible approach.
\end_layout

\begin_layout Itemize
Bayesian: I cannot specify 
\begin_inset Formula $\lambda$
\end_inset

 
\begin_inset Formula $\;\Rightarrow\;$
\end_inset


\begin_inset Formula $\lambda$
\end_inset

 is unknown
\begin_inset Formula $\;\Rightarrow\;$
\end_inset

use a prior for 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Itemize
One possibility: 
\begin_inset Formula $\lambda\sim Inv-\chi^{2}(\eta_{0},\lambda_{0})$
\end_inset

.
 The user specifies 
\begin_inset Formula $\eta_{0}$
\end_inset

 and 
\begin_inset Formula $\lambda_{0}$
\end_inset

.
\end_layout

\begin_layout Itemize
Alternative approach: specify the prior on the 
\emph on
degrees of freedom
\emph default
.
\end_layout

\begin_layout Itemize
Hierarchical setup:
\begin_inset Formula 
\begin{align*}
y|\beta,x & \sim N(x'\beta,\sigma^{2})\\
\beta|\sigma^{2} & \sim N\left(\left(\begin{array}{c}
0\\
0
\end{array}\right),\sigma^{2}\left(\begin{array}{cc}
\delta_{0}^{-1}I_{q} & 0\\
0 & \lambda^{-1}I_{m}
\end{array}\right)\right)\\
\sigma^{2} & \sim Inv-\chi^{2}(\nu_{0},\sigma_{0}^{2})\\
\lambda & \sim Inv-\chi^{2}(\eta_{0},\lambda_{0})
\end{align*}

\end_inset

Note: different shrinkage on the original 
\begin_inset Formula $q$
\end_inset

 covariates (
\begin_inset Formula $\delta_{0}$
\end_inset

) and the covariates that comes from the knots (
\begin_inset Formula $\lambda$
\end_inset

).
\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Estimating the shrinkage, cont.
\end_layout

\begin_layout Itemize
Joint posterior
\begin_inset Formula 
\[
p(\beta,\sigma^{2},\lambda|y,x)=p(\beta,\sigma^{2}|\lambda,y,x)p(\lambda|y,x)
\]

\end_inset

where 
\begin_inset Formula 
\[
p(\lambda|y,x)=\int\int p(\beta,\sigma,\lambda|y,x)d\beta d\sigma^{2}
\]

\end_inset

is the marginal posterior of 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Itemize
The conditional posterior 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $p(\beta,\sigma^{2}|\lambda,y,x)$
\end_inset

 is a special case of our previous results for linear regression with a
 conjugate prior.
 Here 
\begin_inset Formula $\mu_{0}=(0,0)'$
\end_inset

 and
\begin_inset Formula 
\[
\Omega_{0}=\left(\begin{array}{cc}
\delta_{0}I_{q} & 0\\
0 & \lambda I_{m}
\end{array}\right)
\]

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Estimating the shrinkage, cont.
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
The posterior of 
\begin_inset Formula $\beta$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

 conditional on 
\begin_inset Formula $\lambda$
\end_inset

 is therefore
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\beta|\sigma^{2},\lambda,y & \sim N\left[\mu_{n},\sigma^{2}\Omega_{n}^{-1}\right]\\
\sigma^{2}|\lambda,y & \sim Inv-\chi^{2}\left(\nu_{n},\sigma_{n}^{2}\right)
\end{align*}

\end_inset

where 
\begin_inset Formula 
\begin{align*}
\mu_{n} & =\left(X'X+\Omega_{0}\right)^{-1}X'y\\
\Omega_{n} & =X'X+\Omega_{0}\\
\nu_{n} & =\nu_{0}+n\\
\nu_{n}\sigma_{n}^{2} & =\nu_{0}\sigma_{0}^{2}+\left(y'y-\mu_{n}'\Omega_{n}\mu_{n}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
The marginal posterior of 
\begin_inset Formula $\lambda$
\end_inset

 can be shown to be
\begin_inset Formula 
\begin{align*}
p(\lambda|y,x) & \propto\frac{\left|\Omega_{0}\right|^{1/2}}{\left|X'X+\Omega_{0}\right|^{1/2}}\frac{1}{\left(\frac{\nu_{n}\sigma_{n}^{2}}{2}\right)^{\nu_{n}/2}}\cdot p(\lambda),
\end{align*}

\end_inset

where 
\begin_inset Formula $p(\lambda)$
\end_inset

 is the prior for 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout BeginFrame
Summary of the posterior with normal shrinkage prior
\end_layout

\begin_layout Itemize
The joint posterior of 
\begin_inset Formula $\beta$
\end_inset

, 
\begin_inset Formula $\sigma^{2}$
\end_inset

 and 
\begin_inset Formula $\lambda$
\end_inset

 is
\begin_inset Formula 
\begin{align*}
\beta|\sigma^{2},\lambda,y & \sim N\left(\mu_{n},\Omega_{n}^{-1}\right)\\
\sigma^{2}|\lambda,y & \sim Inv-\chi^{2}\left(\nu_{n},\sigma_{n}^{2}\right)\\
p(\lambda|y) & \propto\sqrt{\frac{\left|\Omega_{0}\right|}{\left|X'X+\Omega_{0}\right|}}\left(\frac{\nu_{n}\sigma_{n}^{2}}{2}\right)^{-\nu_{n}/2}\cdot p(\lambda)
\end{align*}

\end_inset

where 
\begin_inset Formula $p(\lambda)$
\end_inset

 is the prior for 
\begin_inset Formula $\lambda$
\end_inset

, and 
\begin_inset Formula 
\begin{align*}
\mu_{n} & =\left(X'X+\Omega_{0}\right)^{-1}X'y\\
\Omega_{n} & =X'X+\Omega_{0}\\
\nu_{n} & =\nu_{0}+n\\
\nu_{n}\sigma_{n}^{2} & =\nu_{0}\sigma_{0}^{2}+y'y-\mu_{n}'\Omega_{n}\mu_{n}
\end{align*}

\end_inset


\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Regularization through Bayesian variable selection
\end_layout

\begin_layout Itemize
Selecting the knots in a spline regression is exactly like variable/covariate
 selection in linear regression.
\end_layout

\begin_layout Itemize
Bayesian variable selection is ideal here.
\end_layout

\begin_layout Itemize
Introduce variable selection indicators, 
\begin_inset Formula $I_{j}$
\end_inset

 such that 
\begin_inset Formula 
\begin{align*}
\beta_{j} & =0 & \text{ if }I_{j}=0\\
\beta_{j} & \sim N(0,\sigma^{2}\lambda^{-1}) & \text{ if }I_{j}=1
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
Need a prior on 
\begin_inset Formula $I_{1},...,I_{K}$
\end_inset

.
 Simple choice: 
\begin_inset Formula $I_{1},...,I_{K}|\theta\overset{iid}{\sim}Bernoulli(\theta)$
\end_inset

.
\end_layout

\begin_layout Itemize
Simulate from the posterior distribution:
\begin_inset Formula 
\[
p(\beta,\sigma^{2},I_{1},...I_{K}|\mathbf{y})=p(\beta,\sigma^{2}|I_{1},...,I_{K},\mathbf{y})p(I_{1},...,I_{K}|\mathbf{y}).
\]

\end_inset


\end_layout

\begin_layout Itemize
Simulate from 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $p(I_{1},...,I_{K}|\mathbf{y})$
\end_inset

 using Gibbs sampling [More later].
\end_layout

\begin_layout Itemize
Automatic model averaging, all in one simulation run.
\end_layout

\begin_layout EndFrame

\end_layout

\begin_layout BeginFrame
Taking it all the way - estimating knot locations
\end_layout

\begin_layout Itemize
The location of the knots can be treated as unknown, and estimated from
 the data.
 This gives a joint posterior
\begin_inset Formula 
\[
p(\beta,\sigma^{2},\lambda,\xi_{1},...,\xi_{q}|y,x)
\]

\end_inset

where 
\begin_inset Formula $\xi_{i}$
\end_inset

 is the location of the 
\begin_inset Formula $i$
\end_inset

th knot.
\end_layout

\begin_layout Itemize
Posterior is complex but can be sampled from by Markov Chain Monte Carlo
 (MCMC).
 Li and Villani (2013, SJS).
\end_layout

\begin_layout EndFrame

\end_layout

\end_body
\end_document
