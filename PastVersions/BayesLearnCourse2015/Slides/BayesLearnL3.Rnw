\batchmode
\makeatletter
\def\input@path{{/home/mv/Dropbox/Teaching/BayesLearning/HT2013/Slides//}}
\makeatother
\documentclass[english,xcolor=svgnames]{beamer}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{amsmath}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
 % this default might be overridden by plain title style
 \newcommand\makebeamertitle{\frame{\maketitle}}%
 \AtBeginDocument{
   \let\origtableofcontents=\tableofcontents
   \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
   \def\gobbletableofcontents#1{\origtableofcontents}
 }
 \def\lyxframeend{} % In case there is a superfluous frame end
 \long\def\lyxframe#1{\@lyxframe#1\@lyxframestop}%
 \def\@lyxframe{\@ifnextchar<{\@@lyxframe}{\@@lyxframe<*>}}%
 \def\@@lyxframe<#1>{\@ifnextchar[{\@@@lyxframe<#1>}{\@@@lyxframe<#1>[]}}
 \def\@@@lyxframe<#1>[{\@ifnextchar<{\@@@@@lyxframe<#1>[}{\@@@@lyxframe<#1>[<*>][}}
 \def\@@@@@lyxframe<#1>[#2]{\@ifnextchar[{\@@@@lyxframe<#1>[#2]}{\@@@@lyxframe<#1>[#2][]}}
 \long\def\@@@@lyxframe<#1>[#2][#3]#4\@lyxframestop#5\lyxframeend{%
   \frame<#1>[#2][#3]{\frametitle{#4}#5}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\setcounter{MaxMatrixCols}{10}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathpazo}
\usepackage{hyperref}
\usepackage{multimedia}
\usepackage{xcolor}
\usepackage{colortbl}
\definecolor{RawSienna}{cmyk}{0,0.87,0.82,0.31}
\definecolor{gray97}{cmyk}{0,0,0,0.03}
\definecolor{robinsegg}{cmyk}{0.18,0.04,0,0.07}
\definecolor{cola}{cmyk}{0,0.315,0.35,0.155}

\newenvironment{stepenumerate}{\begin{enumerate}[<+->]}{\end{enumerate}}
\newenvironment{stepitemize}{\begin{itemize}[<+->]}{\end{itemize} }
\newenvironment{stepenumeratewithalert}{\begin{enumerate}[<+-| alert@+>]}{\end{enumerate}}
\newenvironment{stepitemizewithalert}{\begin{itemize}[<+-| alert@+>]}{\end{itemize} }
\usecolortheme[named=RawSienna]{structure}
%\usecolortheme[RGB={205,0,0}]{structure}
\setbeamertemplate{navigation symbols}{}
\useoutertheme{infolines}
\usetheme{default}
\setbeamertemplate{blocks}[shadow=true]
%\setbeamerfont{structure}{shape=\itshape}
\usefonttheme{structuresmallcapsserif}
\setbeamertemplate{background canvas}{
 % \ifnum \thepage>0 \relax % we are on the first page
%\includegraphics[width=\paperwidth,height=\paperheight]{/home/mv/Dropbox/Foton/IconsWallpaper/greyribbonLighter.jpg}
 % \else
 	% No background for page 2 and onwards
 % \fi
}

\makeatother

\usepackage{babel}
\begin{document}

\title[Bayesian Learning]{Bayesian Learning - Lecture 3}


\author[Mattias Villani]{Mattias Villani}


\institute[\textbf{Statistics, LiU}]{\textbf{Division of Statistics}\\
\textbf{Department of Computer and Information Science}\\
\textbf{Linköping University }}


\date{\,}

\makebeamertitle

\lyxframeend{}


\lyxframeend{}\lyxframe{Lecture overview}
\begin{itemize}
\item Multiparameter models
\item Marginalization
\item Normal model with unknown variance
\item Bayesian analysis of multinomial data
\item Bayesian analysis of multivariate normal data
\end{itemize}

\lyxframeend{}


\lyxframeend{}\lyxframe{Marginalization}
\begin{itemize}
\item Models usually contains several parameter $\theta_{1},\theta_{2},...$.
Examples: $x_{i}\overset{iid}{\sim}N(\theta,\sigma^{2})$; multiple
regression ...
\item The Bayesian computes the joint posterior distribution
\[
p(\theta_{1},\theta_{2},...,\theta_{p}|y)\propto p(y|\theta_{1},\theta_{2},...,\theta_{p})p(\theta_{1},\theta_{2},...,\theta_{p}).
\]
... or in vector form:
\[
p(\theta)\propto p(y|\theta)p(\theta).
\]

\item Complicated to graph the joint posterior.
\item Some of the parameters may not be of direct interest (nuisance parameters),
but are nevertheless needed in the model.
\item No problem: just integrate them out (marginalize with respect to,
average over) all nuisance parameters.
\item Example: $\theta=(\theta_{1},\theta_{2})^{\prime}$, where $\theta_{2}$
is a nuisance. We are interested in the marginal posterior of $\theta_{1}$
\begin{eqnarray*}
p(\theta_{1}|y) & = & \int p(\theta_{1},\theta_{2}|y)d\theta_{2}=\int p(\theta_{1}|\theta_{2},y)p(\theta_{2}|y)d\theta_{2}.
\end{eqnarray*}

\end{itemize}

\lyxframeend{}


\lyxframeend{}\lyxframe{Normal model with unknown variance - Uniform prior}
\begin{itemize}
\item Model:
\[
x_{1},...,x_{n}\overset{iid}{\sim}N(\theta,\sigma^{2})
\]

\item Prior
\[
p(\theta,\sigma^{2})\propto(\sigma^{2})^{-1}
\]

\item Posterior:
\begin{gather*}
\theta|\sigma^{2},\mathbf{x}\sim N\left(\bar{x},\frac{\sigma^{2}}{n}\right)\\
\sigma^{2}|\mathbf{x}\sim\mathrm{Inv}-\chi^{2}(n-1,s^{2}),
\end{gather*}
where
\[
s^{2}=\frac{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}{n-1}
\]
is the usual sample variance.
\end{itemize}

\lyxframeend{}


\lyxframeend{}\lyxframe{Normal model with unknown variance - Uniform prior, cont.}
\begin{itemize}
\item Simulating the posterior of the normal model with non-informative
prior:

\begin{itemize}
\item [1.] Draw $X\sim\chi^{2}(n-1)$
\item [2.] Compute $\sigma^{2}=\frac{(n-1)s^{2}}{X}$ (this a draw from
$\mathrm{Inv}$-$\chi^{2}(n-1,s^{2})$)
\item [3.] Draw a $\theta$ from $N\left(\bar{x},\frac{\sigma^{2}}{n}\right)$
conditional on the previous draw $\sigma^{2}$
\item [4.] Repeat step 1-3 many times. 
\end{itemize}
\item The sampling is implemented in the R program NormalNonInfoPrior.R
\item We may derive the marginal posterior analytically as
\[
\theta|\mathbf{x}\sim t_{n-1}\left(\bar{x},\frac{s^{2}}{n}\right).
\]
 
\end{itemize}

\lyxframeend{}


\lyxframeend{}\lyxframe{Multinomial model with Dirichlet prior}
\begin{itemize}
\item \textit{Data}: $y=(y_{1},...y_{K})$, where $y_{k}$ counts the number
of observations in the $k$th category. $\sum_{k=1}^{K}y_{k}=n$.
Example: brand choices.
\item Multinomial model:
\[
p(y|\theta)\propto\prod_{k=1}^{K}\theta_{k}^{y_{k}},\text{ where }\sum_{k=1}^{K}\theta_{j}=1.
\]

\item \textit{Conjugate prior}: $\mathrm{Dirichlet}(\alpha_{1},...,\alpha_{K})$
\[
p(\theta)\propto\prod_{k=1}^{K}\theta_{j}^{\alpha_{j}-1}.
\]

\item Moments of $\theta=(\theta_{1},...,\theta_{K})'\sim Dirichlet(\alpha_{1},...,\alpha_{K})$
\begin{align*}
\mathrm{E}(\theta_{k}) & =\frac{\alpha_{k}}{\sum_{j=1}^{K}\alpha_{j}}\\
\mathrm{V}(\theta_{k}) & =\frac{\mathrm{E}(\theta_{k})\left[1-\mathrm{E}(\theta_{k})\right]}{1+\sum_{k=1}^{K}\alpha_{k}}
\end{align*}

\item Note that $\sum_{k=1}^{K}\alpha_{k}$ is a precision parameter.
\end{itemize}

\lyxframeend{}


\lyxframeend{}\lyxframe{Multinomial model with Dirichlet prior, cont.}
\begin{itemize}
\item 'Non-informative': $\alpha_{1}=...=\alpha_{K}=1$ (uniform and proper).
\item Simulating from the Dirichlet distribution:

\begin{itemize}
\item Generate $x_{1}\sim Gamma(\alpha_{1},\beta),...,x_{K}\sim Gamma(\alpha_{K},\beta)$,
independently. Any $\beta$ will do as long it is the same for all
$x_{i}$.
\item Compute $y_{k}=x_{k}/(\sum_{j=1}^{K}x_{j})$.
\item $y=(y_{1},...,y_{K})$ is a draw from the $\mathrm{Dirichlet}(\alpha_{1},...,\alpha_{K})$
distribution.
\end{itemize}
\item \textit{Prior-to-Posterior updating}:
\begin{gather*}
Model\text{: \ \ \ \ }y=(y_{1},...y_{K})\sim\mathrm{Multin}(n;\theta_{1},...,\theta_{K})\\
Prior:\text{ \ \ }\theta=(\theta_{1},...,\theta_{K})\sim\mathrm{Dirichlet}(\alpha_{1},...,\alpha_{K})\\
Posterior:\text{ \ \ }\theta|y\sim\mathrm{Dirichlet}(\alpha_{1}+y_{1},...,\alpha_{K}+y_{K}).
\end{gather*}

\end{itemize}

\lyxframeend{}


\lyxframeend{}\lyxframe{Example: market shares}
\begin{itemize}
\item A recent survey among consumer smartphones owners in the U.S. showed
that among the 513 respondents:

\begin{itemize}
\item 180 owned an iPhone
\item 230 owned an Android phone
\item 62 owned a Blackberry phone
\item 41 owned some other mobile phone.
\end{itemize}
\item Previous survey: iPhone 30\%, Android 30\%, Blackberry 20\% and Other
20\%.
\item Pr(Android has largest share | Data)
\item Prior: $\alpha_{1}=15,\alpha_{2}=15,\alpha_{3}=10\text{ and }\alpha_{4}=10$
(prior info is equivalent to a survey with only $50$ respondents)
\item Posterior: \textrm{$(\theta_{1},\theta_{2},\theta_{3},\theta_{4})|\mathbf{y}\sim\mathrm{Dirichlet(179,261,72,51)}$}
\end{itemize}

\lyxframeend{}

\begin{frame}[fragile]
\frametitle{R code for market share example}
<<MultinomialPrior2Post1, eval=TRUE, size='tiny'>>=

# Setting up data and prior
y <- c(180,230,62,41) # The cell phone survey data (K=4)
alpha <- c(15,15,10,10) # Dirichlet prior hyperparameters 
nIter <- 100 # Number of posterior draws

# Defining a function that simulates from a Dirichlet distribution 
SimDirichlet <- function(nIter, param){ 	
  nCat <- length(param) 	
  thetaDraws <- as.data.frame(matrix(NA, nIter, nCat)) # Storage. 	
  for (j in 1:nCat){ 		
    thetaDraws[,j] <- rgamma(nIter,param[j],1) 	
  } 	
  for (i in 1:nIter){ 		
    thetaDraws[i,] = thetaDraws[i,]/sum(thetaDraws[i,])	
  } 	
  return(thetaDraws) 
}

# Posterior sampling from Dirichlet posterior
thetaDraws <- SimDirichlet(nIter,y + alpha)

@
\end{frame}

\begin{frame}[fragile]
\frametitle{R code for market share example, cont}
<<MultinomialPrior2Post2, eval=TRUE, size='tiny'>>=
# Posterior mean and standard deviation of Androids share (in %)
message(mean(100*thetaDraws[,2])) 
message(sd(100*thetaDraws[,2]))

# Computing the posterior probability that Android is the largest
PrAndroidLargest <- sum(thetaDraws[,2] > max(thetaDraws[,c(1,3,4)]))/nIter
message(paste('Pr(Android has the largest market share) = ', PrAndroidLargest))

@
\end{frame}

\begin{frame}[fragile]
\frametitle{R code for market share example, cont}
<<MultinomialPrior2Post3, echo = FALSE, eval=TRUE, out.width = 275 px, out.height = 220 px>>=

# Plots histograms of the posterior draws 
par(mfrow = c(1,2)) # Splits the graphical window in four parts
hist(100*thetaDraws[,1], breaks = 25, main ='iPhone market share (%)')  
hist(100*thetaDraws[,2], breaks = 25, main ='Android market share (%)')

@
\end{frame}


\lyxframeend{}\lyxframe{Multivariate normal - known covariance matrix}
\begin{itemize}
\item Model:
\[
y_{1},...,y_{n}\overset{iid}{\sim}N_{p}(\mu,\Sigma)
\]
where $\Sigma$ is a known covariance matrix.
\item Density
\[
p(y|\mu,\Sigma)=\left|\Sigma\right|^{-1/2}\exp\left(-\frac{1}{2}(y-\mu)'\Sigma^{-1}(y-\mu)\right)
\]

\item Likelihood:
\begin{align*}
p(y_{1},...,y_{n}|\mu,\Sigma) & \propto\left|\Sigma\right|^{-n/2}\exp\left(-\frac{1}{2}\sum_{i=1}^{n}(y_{i}-\mu)'\Sigma^{-1}(y_{i}-\mu)\right)\\
 & =\left|\Sigma\right|^{-n/2}\exp\left(-\frac{1}{2}tr\Sigma^{-1}S_{\mu}\right)
\end{align*}
where $S_{\mu}=\sum_{i=1}^{n}(y_{i}-\mu)(y_{i}-\mu)'.$
\end{itemize}

\lyxframeend{}


\lyxframeend{}\lyxframe{Multivariate normal - known covariance matrix, cont.}
\begin{itemize}
\item Prior:
\[
\mu\sim N_{p}(\mu_{0},\Lambda_{0})
\]

\item Posterior:
\[
\mu|y\sim N(\mu_{n},\Lambda_{n})
\]
where
\begin{align*}
\mu_{n} & =(\Lambda_{0}^{-1}+n\Sigma^{-1})^{-1}(\Lambda_{0}^{-1}\mu_{0}+n\Sigma^{-1}\bar{y})\\
\Lambda_{n}^{-1} & =\Lambda_{0}^{-1}+n\Sigma^{-1}
\end{align*}

\item Note how the posterior mean is (matrix) weighted average of prior
and data information.
\item Noninformative prior: let the precision go to zero: $\Lambda_{0}^{-1}\rightarrow0$.
\end{itemize}

\lyxframeend{}


\lyxframeend{}\lyxframe{Multivariate normal - Conjugate prior}
\begin{itemize}
\item Conjugate prior is Normal-IW$(\mu_{0},\kappa_{0},\Lambda_{0},\nu_{0})$
\begin{align*}
\Sigma & \sim Inv-Wishart(\Lambda_{0},\nu_{0})\\
\mu|\Sigma & \sim N(\mu_{0},\kappa_{0}^{-1}\Sigma)
\end{align*}

\item Density:
\[
\left|\Sigma\right|^{-[(\nu_{0}+p)/2+1]}\exp\left(-\frac{1}{2}tr(\Lambda_{0}\Sigma^{-1})-\frac{\kappa_{0}}{2}(\mu-\mu_{0})'\Sigma^{-1}(\mu-\mu_{0})\right)
\]

\item Posterior is Normal-IW$(\mu_{n},\kappa_{n},\Lambda_{n},\nu_{n})$
\begin{align*}
\mu_{n} & =\frac{\kappa_{0}}{\kappa_{0}+n}\mu_{0}+\frac{n}{\kappa_{0}+n}\bar{y}\\
\kappa_{n} & =\kappa_{0}+n\\
\nu_{n} & =\nu_{0}+n\\
\Lambda_{n} & =\Lambda_{0}+S+\frac{\kappa_{0}n}{\kappa_{0}+n}(\bar{y}-\mu_{0})(\bar{y}-\mu_{0})'
\end{align*}

\end{itemize}

\lyxframeend{}
\end{document}
